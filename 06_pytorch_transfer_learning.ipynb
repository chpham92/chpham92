{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMWB3x+/JTDSPdSm0laYOV5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chpham92/chpham92/blob/main/06_pytorch_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06. PyTorch Transfer Learning\n",
        "\n",
        "What is transfer learning?\n",
        "\n",
        "Transfer learning involves taking the parameters of what one model ahs learned on another dataset and applying to our own problem.\n",
        "\n",
        "* Pretrained model = foundation models\n",
        "\n"
      ],
      "metadata": {
        "id": "XuLZqMvEAzi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "id": "AtcmWgi9BWPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import the code we've written in previous sections so that we don't have to write it all again"
      ],
      "metadata": {
        "id": "UpL_6cXeBiIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine"
      ],
      "metadata": {
        "id": "0xpIOlziCIZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup path to data folder\n",
        "import os\n",
        "import zipfile\n",
        "data_path = \"data\"\n",
        "\n",
        "# Setup train and test directory paths\n",
        "train_dir = os.path.join(data_path, \"train\")\n",
        "test_dir = os.path.join(data_path, \"test\")\n",
        "\n",
        "# Download pizza, steak, sushi images from GitHub if they don't exist\n",
        "if not os.path.exists(train_dir):\n",
        "    print(f\"Did not find {train_dir} directory, downloading data...\")\n",
        "    # Ensure 'data' directory exists before extracting\n",
        "    os.makedirs(data_path, exist_ok=True)\n",
        "    !wget https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\n",
        "    with zipfile.ZipFile(\"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "        zip_ref.extractall(data_path)\n",
        "    os.remove(\"pizza_steak_sushi.zip\")\n",
        "else:\n",
        "    print(f\"Found {train_dir} directory, skipping download.\")\n",
        "\n",
        "# Create a transforms pipeline (this code was imported earlier)\n",
        "# For transfer learning, we often resize images to a consistent size\n",
        "# (e.g., 224x224 for many pre-trained models) and normalize them.\n",
        "# You might need to adjust these based on your specific model's requirements.\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Standard ImageNet normalization\n",
        "])\n",
        "\n",
        "# Define the batch size\n",
        "BATCH_SIZE = 32 # You can adjust this value\n",
        "\n",
        "# Create DataLoaders with the defined variables\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir, test_dir=test_dir, transform=manual_transforms, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Get a pre-trained model (e.g., EfficientNet_B0) and modify its classifier\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # Get the default weights\n",
        "model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
        "\n",
        "# Freeze all base layers in the model\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Change the classifier head to suit our problem\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.2, inplace=True),\n",
        "    nn.Linear(in_features=1280, out_features=len(class_names)) # EfficientNet_B0 has 1280 output features in its head\n",
        ").to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define number of epochs\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Train the model\n",
        "engine.train(model=model, train_dataloader=train_dataloader, test_dataloader=test_dataloader, optimizer=optimizer, loss_fn=loss_fn, epochs=NUM_EPOCHS, device=device)"
      ],
      "metadata": {
        "id": "V0f2eKybDJm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "bg9pTWHcDk_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Get data\n",
        "\n",
        "We need our pizza, steak sushi data to build a transfer learning model on"
      ],
      "metadata": {
        "id": "wWPU0XgPET22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download pizza, steak, sushi data\n",
        "    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "        print(\"Downloading pizza, steak, sushi data...\")\n",
        "        f.write(request.content)\n",
        "\n",
        "    # Unzip pizza, steak, sushi data\n",
        "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "        print(\"Unzipping pizza, steak, sushi data...\")\n",
        "        zip_ref.extractall(image_path)\n",
        "\n",
        "    # Remove .zip file\n",
        "    os.remove(data_path / \"pizza_steak_sushi.zip\")"
      ],
      "metadata": {
        "id": "KqzAcY27EnTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup directory path\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "id": "1bc25eG7FhqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create Datasets and DataLoaders\n",
        "\n",
        "Now we've got some data, want to turn it into PyTorch DataLoaders.\n",
        "\n",
        "To do so, we can use `data_setup.py` and the `create_dataloaders()` function we made in 05. PyTorch Going Modular\n",
        "\n",
        "There's one thing we have to think about when loading: how to **transfor** it?\n",
        "\n",
        "And with `torchvision` 0.13+ there's two ways to do this:\n",
        "\n",
        "1. Manually created transforms - you define what transforms you want your data to go through.\n",
        "2. Automatically created transforms - the transforms for your data are defined by the model you'd like to see\n",
        "\n",
        "Important point: When using a pretrained model, it's important that the data (including your custom data) that you pass through it is *transformed* in the same way that the data the model was trained on"
      ],
      "metadata": {
        "id": "r3Kkp_HzF2PO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import data_setup\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir, test_dir=test_dir, transform=manual_transforms, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "GUF84HuoGEQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision"
      ],
      "metadata": {
        "id": "tFNUYJyHHtFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Creating a transform for `torchvision.models` (manual creation)\n",
        "\n",
        "`torchvision.models` contains pretrained models (models ready for transfer learning) right within `torchvision`"
      ],
      "metadata": {
        "id": "LnUWPRHMH64X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "# Create a transforms pipeline manually (required for torchvision < 0.13)\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (heightxwidth, though some models may require different sizes)\n",
        "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
        "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
        "])"
      ],
      "metadata": {
        "id": "iB2-LGnZIDcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import data_setup\n",
        "# Create training and testing DataLoaders as well as get a list of class names\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=manual_transforms, # resize, convert images to between 0 & 1 and normalize them\n",
        "                                                                               batch_size=32) # set mini-batch size to 32\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "JXVD2wq9JbZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Creating a transform for `torchvision.models` (auto creation)\n",
        "\n",
        "As of `torchvision` v0.13+ there is now support for automatic data transform creation based on the pretrained model weights you're using"
      ],
      "metadata": {
        "id": "3uZa_GRfJ9bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a set of pretrained model weights\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "\n",
        "weights"
      ],
      "metadata": {
        "id": "y6erF5IYKoc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the transforms used to create our pretrained weights\n",
        "auto_transforms = weights.transforms()\n",
        "auto_transforms"
      ],
      "metadata": {
        "id": "WJDWgUQbL5oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders using automatic transforms\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=auto_transforms, # perform same data transforms on our own data as the pretrained model\n",
        "                                                                               batch_size=32) # set mini-batch size to 32\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "QTqIJWV-MEmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Getting a pretrained model\n",
        "\n",
        "There are various places to get a pretrained model, such as:\n",
        "\n",
        "1. PyTorch domain libraries\n",
        "2. Libraries like `timm` (torch image models)\n",
        "3. HuggingFace Hub (for plenty of different models)\n",
        "4. Paperswithcode (for models across different problem spaces/domains)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FiItGBnSMQua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Which pretrained model should you choose?\n",
        "\n",
        "Experiment!\n",
        "\n",
        "The whole idea of transfer learning: take an already well-performing model from a problem space similar to your own and then customize to your own problem.\n",
        "\n",
        "Three things to ocnsider:\n",
        "1. Speed - how fast does it run?\n",
        "2. Size - how big is the model?\n",
        "3. Performance - how well deos it go on your chosen problem\n",
        "\n",
        "Where does the model live?\n",
        "Is it on a device (like a self-driving car)\n",
        "Or does it live in a server?\n",
        "\n",
        "Which model should we choose?\n",
        "\n",
        "For our case, it looks like EffNetB0 is one of our best options in terms performance vs size.\n",
        "\n",
        "However, in light of the Bitter Lesson, if we had infinite compute, we'd likely pick the biggest model + most parameters + most general we could."
      ],
      "metadata": {
        "id": "Gr0ATRnZNUOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Setting up a pretrained model\n",
        "\n",
        "Want to create an instance of a pretrained EffNetB0"
      ],
      "metadata": {
        "id": "mIa8uPSLN20r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OLD: Setup the model with pretrained weights and send it to the target device (this was prior to torchvision v0.13)\n",
        "# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD method (with pretrained=True)\n",
        "\n",
        "# NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights\n",
        "model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
        "model\n",
        "#model # uncomment to output (it's very long)"
      ],
      "metadata": {
        "id": "f0V8fujpFz00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.features"
      ],
      "metadata": {
        "id": "Wa-rbpR-GZW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.classifier"
      ],
      "metadata": {
        "id": "JVHtVxVhGukA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Getting a summary of of model with `torchinfo.summary()`"
      ],
      "metadata": {
        "id": "wTHdPUocGxcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a summary using torchinfo (uncomment for actual output)\n",
        "from torchinfo import summary\n",
        "summary(model=model,\n",
        "        input_size=(1, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
        "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ],
      "metadata": {
        "id": "YwOyRTafKEga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Freezing the base model and changing the output layer to suit our needs\n",
        "\n",
        "With a feature extractor model, typically you will \"freeze\" the base layers of a pretrained/foundation model and update the out put layers to suit your own problem."
      ],
      "metadata": {
        "id": "m57gr4hYKcW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "B3EZPc7aLn7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import Linear\n",
        "# Update the classifier head of our model to suit our problem\n",
        "from torch import nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.2, inplace=True),\n",
        "    Linear(in_features=1280, out_features=len(class_names))\n",
        ").to(device)\n",
        "model.classifier"
      ],
      "metadata": {
        "id": "IX2NxxPMNNK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a summary using torchinfo (uncomment for actual output)\n",
        "from torchinfo import summary\n",
        "summary(model=model,\n",
        "        input_size=(1, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
        "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ],
      "metadata": {
        "id": "aL8iIdj5LwdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Train model"
      ],
      "metadata": {
        "id": "0ppRSekVMTr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "cfq2ioZQPzre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import train function\n",
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Set the manual seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "# Setup training and save the results\n",
        "results = engine.train(model=model,\n",
        "                       train_dataloader=train_dataloader,\n",
        "                       test_dataloader=test_dataloader,\n",
        "                       optimizer=optimizer,\n",
        "                       loss_fn=loss_fn,\n",
        "                       epochs=5,\n",
        "                       device=device)\n",
        "# end the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n",
        "\n"
      ],
      "metadata": {
        "id": "s9ioHfyQP4ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Evaluate model by plotting loss curve"
      ],
      "metadata": {
        "id": "gtuXSe4FQXk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the plot_loss_curves() function from helper_functions.py, download the file if we don't have it\n",
        "try:\n",
        "    from helper_functions import plot_loss_curves\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find helper_functions.py, downloading...\")\n",
        "    with open(\"helper_functions.py\", \"wb\") as f:\n",
        "        import requests\n",
        "        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "        f.write(request.content)\n",
        "    from helper_functions import plot_loss_curves\n",
        "\n",
        "# Plot the loss curves of our model\n",
        "plot_loss_curves(results)"
      ],
      "metadata": {
        "id": "DCvgUmhCROV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Make predictions on images from the test set\n",
        "\n",
        "Some things to keep in mind when makeing predictions/inference on test data/custom data.\n",
        "\n",
        "We have to make sure that our test/custom data is:\n",
        "* Same shape\n",
        "* Same datatype\n",
        "* Same device\n",
        "* Same transform\n",
        "\n",
        "To do all of this automatically, let's create a function called `pred_and_plot_image()`\n",
        "\n",
        "1. Take in a trained model, a list of class names, a filepath to a target image, image size, a transform and a target device\n",
        "2. Open the image wit `PIL.Image.Open()`\n",
        "3. Create a transform if one doesn't exist\n",
        "4. Make sure the model is on the target device\n",
        "5. Turn the model to `model.eval()` mode to make sure it's ready for inference\n",
        "6. Transform the target image and make sure its dimensionality is suited for the model (this mainly relates to batch size)\n",
        "7. Make a prediction on the image by passing to the model\n",
        "8. Convert the model's output logits to prediction probabilities using `torch.softmax()`\n",
        "9. Convert model's prediction probabilties to predition labels using `torch.argmax()`\n",
        "10. Plot the image with `matplotlib` and set the title to the prediction label from step 9 and prediction probability from step 8"
      ],
      "metadata": {
        "id": "2FpIwUJQRlxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# 1. Take in a trained model, class names, image path, image size, a transform and target device\n",
        "def pred_and_plot_image(model: torch.nn.Module,\n",
        "                        image_path: str,\n",
        "                        class_names: List[str],\n",
        "                        image_size: Tuple[int, int] = (224, 224),\n",
        "                        transform: torchvision.transforms = None,\n",
        "                        device: torch.device=device):\n",
        "\n",
        "\n",
        "    # 2. Open image\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # 3. Create transformation for image (if one doesn't exist)\n",
        "    if transform is not None:\n",
        "        image_transform = transform\n",
        "    else:\n",
        "        image_transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    ### Predict on image ###\n",
        "\n",
        "    # 4. Make sure the model is on the target device\n",
        "    model.to(device)\n",
        "\n",
        "    # 5. Turn on model evaluation mode and inference mode\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
        "      transformed_image = image_transform(img).unsqueeze(dim=0)\n",
        "\n",
        "      # 7. Make a prediction on image with an extra dimension and send it to the target device\n",
        "      target_image_pred = model(transformed_image.to(device))\n",
        "\n",
        "    # 8. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
        "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
        "\n",
        "    # 9. Convert prediction probabilities -> prediction labels\n",
        "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
        "\n",
        "    # 10. Plot image with predicted label and probability\n",
        "    plt.figure()\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")\n",
        "    plt.axis(False);"
      ],
      "metadata": {
        "id": "Fj7_zG6ZaVaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a random list of image paths from test set\n",
        "import random\n",
        "num_images_to_plot = 3\n",
        "test_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data\n",
        "test_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n",
        "                                       k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n",
        "\n",
        "# Make predictions on and plot the images\n",
        "for image_path in test_image_path_sample:\n",
        "    pred_and_plot_image(model=model,\n",
        "                        image_path=image_path,\n",
        "                        class_names=class_names,\n",
        "                        # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n",
        "                        image_size=(224, 224))"
      ],
      "metadata": {
        "id": "p0wwgjtJdTRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 6.1 Making predictions on a custom image\n",
        "# Download custom image\n",
        "import requests\n",
        "\n",
        "# Setup custom image path\n",
        "custom_image_path = data_path / \"04-pizza-dad.jpeg\"\n",
        "\n",
        "# Download the image if it doesn't already exist\n",
        "if not custom_image_path.is_file():\n",
        "    with open(custom_image_path, \"wb\") as f:\n",
        "        # When downloading from GitHub, need to use the \"raw\" file link\n",
        "        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n",
        "        print(f\"Downloading {custom_image_path}...\")\n",
        "        f.write(request.content)\n",
        "else:\n",
        "    print(f\"{custom_image_path} already exists, skipping download.\")\n",
        "\n",
        "# Predict on custom image\n",
        "pred_and_plot_image(model=model,\n",
        "                    image_path=custom_image_path,\n",
        "                    class_names=class_names)"
      ],
      "metadata": {
        "id": "te0Bn-Z_dh00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zsTxJ3UJeiVH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}