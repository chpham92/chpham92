{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMWLH1clCbDI94Ghaed9w2A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chpham92/chpham92/blob/main/09_pytorch_model_deploy_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 09. PyTorch Model Deployment\n",
        "\n",
        "What is model deployment?\n",
        "\n",
        "Machine learning model deployment is the act of making your machine learning model(s) available to someone or soemthing else\n",
        "\n"
      ],
      "metadata": {
        "id": "GW4jupRzKKl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Get setup"
      ],
      "metadata": {
        "id": "Y_aos1uPKzI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Getting data\n",
        "\n",
        "The dataset we're going to use for deploying a FoodVision Mini model is..\n",
        "\n",
        "Pizza, steak, sushi 20% dataset (pizza, steak, sushi classes from Food101, random 20% of samples)\n",
        "\n",
        "Data can be downloaded from:\n",
        "https://www.learnpytorch.io/09_pytorch_model_deployment/"
      ],
      "metadata": {
        "id": "-bHsgP9jK5jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "6roMdoRqdBZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "# try:\n",
        "#     import torch\n",
        "#     import torchvision\n",
        "#     assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "#     assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "#     print(f\"torch version: {torch.__version__}\")\n",
        "#     print(f\"torchvision version: {torchvision.__version__}\")\n",
        "# except:\n",
        "#     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "#     !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "#     import torch\n",
        "#     import torchvision\n",
        "#     print(f\"torch version: {torch.__version__}\")\n",
        "#     print(f\"torchvision version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "id": "2ywB0UhtLXZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ],
      "metadata": {
        "id": "PkrDMFpbLdBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "2yJzJwozLj-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pizza, steak, sushi images from GitHub\n",
        "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
        "                                     destination=\"pizza_steak_sushi_20_percent\")\n",
        "\n",
        "data_20_percent_path"
      ],
      "metadata": {
        "id": "ici6O4o9MB72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup training and test paths\n",
        "train_dir = data_20_percent_path / \"train\"\n",
        "test_dir = data_20_percent_path / \"test\""
      ],
      "metadata": {
        "id": "3dJlh2cAMSTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. FoodVision Mini model deployment experiment outline\n",
        "\n",
        "### 3 Questions\n",
        "1. What is my most ideal machine learning model deployment scenario?\n",
        "2. Where is my model going to go?\n",
        "3. How is my model going to function?\n",
        "\n",
        "**FoodVision Mini ideal use case** A model that performs well and fast.\n",
        "\n",
        "* Performs well: 95%+ accuracy\n",
        "* Fast: as close to real-time (or faster) as possible (30FPS+ or 30ms latency)\n",
        "  * Latency=time for preeiction to take place\n",
        "\n",
        "To try and achieve these goals, we're going to build two model experiments:\n",
        "\n",
        "1. EffNetB2 feature extractor\n",
        "2. ViT feature extractor"
      ],
      "metadata": {
        "id": "XGpNGk6CMq4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Creating an EffNetB2 feature extractor\n",
        "\n",
        "Feature extractor = a term for a transfer learning model that has its base layers frozen and output layers (or head layers) customized to a certain problem."
      ],
      "metadata": {
        "id": "vX0iuiQ0NmzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup pretrained EffNetB2 weights\n",
        "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "\n",
        "# 2. Get EffNetB2 transforms\n",
        "effnetb2_transforms = effnetb2_weights.transforms()\n",
        "\n",
        "# 3. Setup pretrained model\n",
        "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"\n",
        "\n",
        "# 4. Freeze the base layers in the model (this will freeze all layers to begin with)\n",
        "for param in effnetb2.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "QbKjJXarOcmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out EffNetB2 classifier head\n",
        "effnetb2.classifier"
      ],
      "metadata": {
        "id": "uisjJzLxPqS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seeds for reproducibility\n",
        "set_seeds()\n",
        "# 5. Update the classifier head\n",
        "effnetb2.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.3, inplace=True), # keep dropout layer same\n",
        "    nn.Linear(in_features=1408, # keep in_features same\n",
        "              out_features=3)) # change out_features to suit our number of classes"
      ],
      "metadata": {
        "id": "0_8SZnHJPuQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Creating a function to make an EffNetB2 feature extractor"
      ],
      "metadata": {
        "id": "3n5x5n6pR-EI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_effnetb2_model(num_classes:int=3, # default output classes = 3 (pizza, steak, sushi)\n",
        "                          seed:int=42):\n",
        "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int, optional): number of classes in the classifier head.\n",
        "            Defaults to 3.\n",
        "        seed (int, optional): random seed value. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): EffNetB2 feature extractor model.\n",
        "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
        "    \"\"\"\n",
        "    # 1, 2, 3. Create EffNetB2 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "    # 4. Freeze all layers in base model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 5. Change classifier head with random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.3, inplace=True),\n",
        "        nn.Linear(in_features=1408, out_features=num_classes),\n",
        "    )\n",
        "\n",
        "    return model, transforms"
      ],
      "metadata": {
        "id": "QYPWYA3HQmMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,\n",
        "                                                      seed=42)"
      ],
      "metadata": {
        "id": "cUT0syCDRX3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# # Print EffNetB2 model summary (uncomment for full output)\n",
        "# summary(effnetb2,\n",
        "#         input_size=(1, 3, 224, 224),\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "7Qqod4GoRlnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Creating DataLoaders for EffNetB2"
      ],
      "metadata": {
        "id": "iiKcyxCtRu_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup DataLoaders\n",
        "from going_modular.going_modular import data_setup\n",
        "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                                 test_dir=test_dir,\n",
        "                                                                                                 transform=effnetb2_transforms,\n",
        "                                                                                                 batch_size=32)"
      ],
      "metadata": {
        "id": "skzcBkDnSiTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Training EffNetB2 feture extractor"
      ],
      "metadata": {
        "id": "YkgMW8e_TKgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
        "                             lr=1e-3)\n",
        "# Setup loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Set seeds for reproducibility and train the model\n",
        "set_seeds()\n",
        "effnetb2_results = engine.train(model=effnetb2,\n",
        "                                train_dataloader=train_dataloader_effnetb2,\n",
        "                                test_dataloader=test_dataloader_effnetb2,\n",
        "                                epochs=10,\n",
        "                                optimizer=optimizer,\n",
        "                                loss_fn=loss_fn,\n",
        "                                device=device)"
      ],
      "metadata": {
        "id": "q0c-Egt8TUBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Inspecting EffNetB2 loss curves"
      ],
      "metadata": {
        "id": "sGUXzpibUaly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(effnetb2_results)"
      ],
      "metadata": {
        "id": "tptX7PuGU603"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Saving EffNetB2 feature extractor"
      ],
      "metadata": {
        "id": "S8hvIeNHVDq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import utils\n",
        "\n",
        "# Save the model\n",
        "utils.save_model(model=effnetb2,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
      ],
      "metadata": {
        "id": "Ji5bLzjTVOb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Checking the size of EffNetB2 feature extractor\n",
        "\n",
        "Why would it be important to consier the size of a saved model?\n",
        "\n",
        "If we're deploying our model to be used on a mobile app/website, there may be limited cmomputer resources.\n",
        "\n",
        "So if our model file is too large, we may not be able to store/run it on our target device."
      ],
      "metadata": {
        "id": "kzP0330GVrS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\n",
        "print(f\"Pretrained EffNetB2 feature extractor model size: {pretrained_effnetb2_model_size} MB\")"
      ],
      "metadata": {
        "id": "PsC077wtW1Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 Collecting EffNetB2 feature extractor stats"
      ],
      "metadata": {
        "id": "epbuuLotV0Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of parameters in EffNetB2\n",
        "effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\n",
        "effnetb2_total_params"
      ],
      "metadata": {
        "id": "0rRgSsejXILh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary with EffNetB2 statistics\n",
        "effnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],\n",
        "                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n",
        "                  \"number_of_parameters\": effnetb2_total_params,\n",
        "                  \"model_size (MB)\": pretrained_effnetb2_model_size}\n",
        "effnetb2_stats"
      ],
      "metadata": {
        "id": "cH2PVDqUXeS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Creating a ViT feature extractor\n",
        "\n",
        "We're up for our second modelling experiment, repeating the steps for EffNetB2 but this time with aa ViT feture extractor."
      ],
      "metadata": {
        "id": "Zts6wswMYkJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out ViT heads layer\n",
        "vit = torchvision.models.vit_b_16()\n",
        "vit.heads"
      ],
      "metadata": {
        "id": "9ZGIIRGAZQwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vit_model(num_classes:int=3,\n",
        "                     seed:int=42):\n",
        "    \"\"\"Creates a ViT-B/16 feature extractor model and transforms.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int, optional): number of target classes. Defaults to 3.\n",
        "        seed (int, optional): random seed value for output layer. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): ViT-B/16 feature extractor model.\n",
        "        transforms (torchvision.transforms): ViT-B/16 image transforms.\n",
        "    \"\"\"\n",
        "    # Create ViT_B_16 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.vit_b_16(weights=weights)\n",
        "\n",
        "    # Freeze all layers in model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Change classifier head to suit our needs (this will be trainable)\n",
        "    torch.manual_seed(seed)\n",
        "    model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model\n",
        "                                          out_features=num_classes)) # update to reflect target number of classes\n",
        "\n",
        "    return model, transforms"
      ],
      "metadata": {
        "id": "nIPgf63IhHoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ViT model and transforms\n",
        "vit, vit_transforms = create_vit_model(num_classes=3,\n",
        "                                       seed=42)\n",
        "vit_transforms"
      ],
      "metadata": {
        "id": "SPRimfSliXP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# # Print ViT feature extractor model summary (uncomment for full output)\n",
        "# summary(vit,\n",
        "#         input_size=(1, 3, 224, 224),\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "Jo_-l-SJjdLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Create DataLoaders for ViT feature extractor"
      ],
      "metadata": {
        "id": "tsEvJxCHjGQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup ViT DataLoaders\n",
        "from going_modular.going_modular import data_setup\n",
        "train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                       test_dir=test_dir,\n",
        "                                                                                       transform=vit_transforms,\n",
        "                                                                                       batch_size=32)"
      ],
      "metadata": {
        "id": "EkGd4dLhjbGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Training ViT feature extractor"
      ],
      "metadata": {
        "id": "IvT-rw-1j9Py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
        "                             lr=1e-3)\n",
        "# Setup loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train ViT model with seeds set for reproducibility\n",
        "set_seeds()\n",
        "vit_results = engine.train(model=vit,\n",
        "                           train_dataloader=train_dataloader_vit,\n",
        "                           test_dataloader=test_dataloader_vit,\n",
        "                           epochs=10,\n",
        "                           optimizer=optimizer,\n",
        "                           loss_fn=loss_fn,\n",
        "                           device=device)"
      ],
      "metadata": {
        "id": "_JQXYXrSkFPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Inspecting ViT loss curves"
      ],
      "metadata": {
        "id": "6ESFXBFZkIJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(vit_results)"
      ],
      "metadata": {
        "id": "wFDjLf7FkUcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Saving ViT feature extractor"
      ],
      "metadata": {
        "id": "CL9gES7qkfzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "from going_modular.going_modular import utils\n",
        "\n",
        "utils.save_model(model=vit,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
      ],
      "metadata": {
        "id": "sJZ97D0Dks5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Checking the size of ViT feature extractor"
      ],
      "metadata": {
        "id": "KRUVgeOilF0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\n",
        "print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")"
      ],
      "metadata": {
        "id": "G0QHATeBlLYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6 Collecting ViT feature stats"
      ],
      "metadata": {
        "id": "m_K1GByjl1ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of parameters in ViT\n",
        "vit_total_params = sum(torch.numel(param) for param in vit.parameters())\n",
        "vit_total_params"
      ],
      "metadata": {
        "id": "rPPRr0NHmFVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ViT statistics dictionary\n",
        "vit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],\n",
        "             \"test_acc\": vit_results[\"test_acc\"][-1],\n",
        "             \"number_of_parameters\": vit_total_params,\n",
        "             \"model_size (MB)\": pretrained_vit_model_size}\n",
        "\n",
        "vit_stats"
      ],
      "metadata": {
        "id": "rizCOlBLmHgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Making predictions with our trained models and timing them"
      ],
      "metadata": {
        "id": "gZbYrJvOmOaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get all test data paths\n",
        "print(f\"[INFO] Finding all filepaths ending with '.jpg' in directory: {test_dir}\")\n",
        "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "test_data_paths[:5]"
      ],
      "metadata": {
        "id": "jye1hO_9mc2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Creating a function to make predictions across the test dataset\n",
        "\n",
        "To create our `pred_and_store()` function:\n",
        "\n",
        "1. Create a function that takes a list of paths, a trained PyTorch model, a series of transforms (to prepare images), a list of target class names and a target device.\n",
        "2. Create an empty list to store prediction dictionaries (we want the function to return a list of dictionaries, one for each prediction).\n",
        "3. Loop through the target input paths (steps 4-14 will happen inside the loop).\n",
        "4. Create an empty dictionary for each iteration in the loop to store prediction values per sample.\n",
        "5. Get the sample path and ground truth class name (we can do this by inferring the class from the path).\n",
        "6. Start the prediction timer using Python's `timeit.default_timer()`.\n",
        "7. Open the image using `PIL.Image.open(path)`.\n",
        "8. Transform the image so it's capable of being used with the target model as well as add a batch dimension and send the image to the target device.\n",
        "9. Prepare the model for inference by sending it to the target device and turning on eval() mode.\n",
        "10. Turn on `torch.inference_mode()` and pass the target transformed image to the model and calculate the prediction probability using torch.softmax() and the target label using torch.argmax().\n",
        "11. Add the prediction probability and prediction class to the prediction dictionary created in step 4. Also make sure the prediction probability is on the CPU so it can be used with non-GPU libraries such as NumPy and pandas for later inspection.\n",
        "12. End the prediction timer started in step 6 and add the time to the prediction dictionary created in step 4.\n",
        "13. See if the predicted class matches the ground truth class from step 5 and add the result to the prediction dictionary created in step 4.\n",
        "14. Append the updated prediction dictionary to the empty list of predictions created in step 2.\n",
        "15. Return the list of prediction dictionaries."
      ],
      "metadata": {
        "id": "XsHXaNyum3-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List, Dict\n",
        "\n",
        "# 1. Create a function to return a list of dictionaries with sample, truth label, prediction, prediction probability and prediction time\n",
        "def pred_and_store(paths: List[pathlib.Path],\n",
        "                   model: torch.nn.Module,\n",
        "                   transform: torchvision.transforms,\n",
        "                   class_names: List[str],\n",
        "                   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -> List[Dict]:\n",
        "\n",
        "    # 2. Create an empty list to store prediction dictionaries\n",
        "    pred_list = []\n",
        "\n",
        "    # 3. Loop through target paths\n",
        "    for path in tqdm(paths):\n",
        "\n",
        "        # 4. Create empty dictionary to store prediction information for each sample\n",
        "        pred_dict = {}\n",
        "\n",
        "        # 5. Get the sample path and ground truth class name\n",
        "        pred_dict[\"image_path\"] = path\n",
        "        class_name = path.parent.stem\n",
        "        pred_dict[\"class_name\"] = class_name\n",
        "\n",
        "        # 6. Start the prediction timer\n",
        "        start_time = timer()\n",
        "\n",
        "        # 7. Open image path\n",
        "        img = Image.open(path)\n",
        "\n",
        "        # 8. Transform the image, add batch dimension and put image on target device\n",
        "        transformed_image = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "        # 9. Prepare model for inference by sending it to target device and turning on eval() mode\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        # 10. Get prediction probability, predicition label and prediction class\n",
        "        with torch.inference_mode():\n",
        "            pred_logit = model(transformed_image) # perform inference on target sample\n",
        "            pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probabilities\n",
        "            pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probabilities into prediction label\n",
        "            pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU\n",
        "\n",
        "            # 11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on)\n",
        "            pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n",
        "            pred_dict[\"pred_class\"] = pred_class\n",
        "\n",
        "            # 12. End the timer and calculate time per pred\n",
        "            end_time = timer()\n",
        "            pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)\n",
        "\n",
        "        # 13. Does the pred match the true label?\n",
        "        pred_dict[\"correct\"] = class_name == pred_class\n",
        "\n",
        "        # 14. Add the dictionary to the list of preds\n",
        "        pred_list.append(pred_dict)\n",
        "\n",
        "    # 15. Return list of prediction dictionaries\n",
        "    return pred_list"
      ],
      "metadata": {
        "id": "2g4XI32YnIep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Making and timing predictions with EffNetB2\n",
        "\n",
        "Let's test our `pred_and_store()` function\n",
        "\n",
        "Two things to note:\n",
        "1. Device - we're going to hardcode our predictions to happen on CPU (because you won't always be sure of having a GPU when you deploy your model).\n",
        "2. Transforms - we want to make sure ach of the models are predicting on images that have been prepared with the appropriate transforms (e.g. EffNetB2 with effnetb22_transforms)"
      ],
      "metadata": {
        "id": "z7kUQbqVrTd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions across test dataset with EffNetB2\n",
        "effnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
        "                                          model=effnetb2,\n",
        "                                          transform=effnetb2_transforms,\n",
        "                                          class_names=class_names,\n",
        "                                          device=\"cpu\") # make predictions on CPU"
      ],
      "metadata": {
        "id": "GLVOt3FbsGC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the first 2 prediction dictionaries\n",
        "effnetb2_test_pred_dicts[:2]"
      ],
      "metadata": {
        "id": "lMazKiJ4sRhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the test_pred_dicts into a DataFrame\n",
        "import pandas as pd\n",
        "effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\n",
        "effnetb2_test_pred_df.head()"
      ],
      "metadata": {
        "id": "T1wENC2_sWwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check number of correct predictions\n",
        "effnetb2_test_pred_df.correct.value_counts()"
      ],
      "metadata": {
        "id": "dSnf5j6ash6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the average time per prediction\n",
        "effnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4)\n",
        "print(f\"EffNetB2 average time per prediction: {effnetb2_average_time_per_pred} seconds\")"
      ],
      "metadata": {
        "id": "1kLU-e1ObQJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add EffNetB2 average prediction time to stats dictionary\n",
        "effnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred\n",
        "effnetb2_stats"
      ],
      "metadata": {
        "id": "Wa5qR89zbtBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Making and timing predictions on ViT"
      ],
      "metadata": {
        "id": "mnphelLacFUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make list of prediction dictionaries with ViT feature extractor model on test\n",
        "vit_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
        "                                     model=vit,\n",
        "                                     transform=vit_transforms,\n",
        "                                     class_names=class_names,\n",
        "                                     device=\"cpu\")"
      ],
      "metadata": {
        "id": "G_eOQ92qcI4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first couple of ViT preedictions on the test dataset\n",
        "vit_test_pred_dicts[:2]"
      ],
      "metadata": {
        "id": "lKjnnE-Vc7_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn vit_test_pred_dicts into a DataFrame\n",
        "import pandas as pd\n",
        "vit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\n",
        "vit_test_pred_df.head()"
      ],
      "metadata": {
        "id": "cw-cGJjBdYwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of correct predictions\n",
        "vit_test_pred_df.correct.value_counts()"
      ],
      "metadata": {
        "id": "DnQUYzNRdw9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average time per prediction for ViT model\n",
        "vit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4)\n",
        "print(f\"ViT average time per prediction: {vit_average_time_per_pred} seconds\")"
      ],
      "metadata": {
        "id": "JxVwhkuHd9xE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add average prediction time for ViT model on CPU\n",
        "vit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred\n",
        "vit_stats"
      ],
      "metadata": {
        "id": "jhZDQxuwecd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Comparing model results, prediction times and size"
      ],
      "metadata": {
        "id": "WCGpyZkYe8Sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn stat dictionaries into DataFrame\n",
        "df = pd.DataFrame([effnetb2_stats, vit_stats])\n",
        "\n",
        "# Add column for model names\n",
        "df[\"model\"] = [\"EffNetB2\", \"ViT\"]\n",
        "\n",
        "# Convert accuracy to percentages\n",
        "df[\"test_acc\"] = round(df[\"test_acc\"] * 100, 2)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "KV91HvltMeWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare ViT to EffNetB2 across different characteristics\n",
        "pd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"]), # divide ViT statistics by EffNetB2 statistics\n",
        "             columns=[\"ViT to EffNetB2 ratios\"]).T"
      ],
      "metadata": {
        "id": "Yxpwc8biMfBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems our ViT model outperforms the EffNetB2 model across the performance metrics (test loss, where lower is better and test accuracy, where higher is better) but at the expense of having:\n",
        "\n",
        "* 11x+ the number of parameters.\n",
        "* 11x+ the model size.\n",
        "* 2.5x+ the prediction time per image.\n",
        "\n",
        "Are these tradeoffs worth it?\n",
        "\n",
        "Perhaps if we had unlimited compute power but for our use case of deploying the FoodVision Mini model to a smaller device (e.g. a mobile phone), we'd likely start out with the EffNetB2 model for faster predictions at a slightly reduced performance but dramatically smaller size."
      ],
      "metadata": {
        "id": "j3cfxAagi0Az"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Visualizing the speed vs. performance tradeoff\n",
        "\n",
        "We've seen that our ViT model outperforms our EffNetB2 model in terms of performance metrics such as test loss and test accuracy.\n",
        "\n",
        "However, our EffNetB2 model performs predictions faster and has a much smaller model size.\n",
        "\n",
        "We can do so by creating a plot with matplotlib:\n",
        "\n",
        "1. Create a scatter plot from the comparison DataFrame to compare EffNetB2 and ViT `time_per_pred_cpu` and `test_acc` values.\n",
        "2. Add titles and labels respective of the data and customize the fontsize for aesthetics.\n",
        "3. Annotate the samples on the scatter plot from step 1 with their appropriate labels (the model names).\n",
        "4. Create a legend based on the model sizes (`model_size (MB)`)."
      ],
      "metadata": {
        "id": "E9xO9ruINWun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a plot from model comparison DataFrame\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "scatter = ax.scatter(data=df,\n",
        "                     x=\"time_per_pred_cpu\",\n",
        "                     y=\"test_acc\",\n",
        "                     c=[\"blue\", \"orange\"], # what colours to use?\n",
        "                     s=\"model_size (MB)\") # size the dots by the model sizes\n",
        "\n",
        "# 2. Add titles, labels and customize fontsize for aesthetics\n",
        "ax.set_title(\"FoodVision Mini Inference Speed vs Performance\", fontsize=18)\n",
        "ax.set_xlabel(\"Prediction time per image (seconds)\", fontsize=14)\n",
        "ax.set_ylabel(\"Test accuracy (%)\", fontsize=14)\n",
        "ax.tick_params(axis='both', labelsize=12)\n",
        "ax.grid(True)\n",
        "\n",
        "# 3. Annotate with model names\n",
        "for index, row in df.iterrows():\n",
        "    ax.annotate(text=row[\"model\"], # note: depending on your version of Matplotlib, you may need to use \"s=...\" or \"text=...\", see: https://github.com/faustomorales/keras-ocr/issues/183#issuecomment-977733270\n",
        "                xy=(row[\"time_per_pred_cpu\"]+0.0006, row[\"test_acc\"]+0.03),\n",
        "                size=12)\n",
        "\n",
        "# 4. Create a legend based on model sizes\n",
        "handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5)\n",
        "model_size_legend = ax.legend(handles,\n",
        "                              labels,\n",
        "                              loc=\"lower right\",\n",
        "                              title=\"Model size (MB)\",\n",
        "                              fontsize=12)\n",
        "\n",
        "# Save the figure\n",
        "!mdkir images/\n",
        "plt.savefig(\"images/09-foodvision-mini-inference-speed-vs-performance.jpg\")\n",
        "\n",
        "# Show the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ublv0K2bNfrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot really visualizes the **speed vs. performance** tradeoff, in other words, when you have a larger, better performing deep model (like our ViT model), it generally takes longer to perform inference (higher latency)."
      ],
      "metadata": {
        "id": "E8r6PnVxRFqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Bringing FoodVision Mini to life by creating a Gradio demo\n",
        "\n",
        "\"Gradio is the fastest way to demo your machine learning model with a friendly web interface so that anyone can use it, anywhere!\"\n",
        "\n",
        "Because metrics on the test set look nice but you never really know how your model performs until you use it in the wild.\n",
        "\n",
        "We'll start by importing Gradio with the common alias `gr` and if it's not present, we'll install it."
      ],
      "metadata": {
        "id": "_RaqBzYpRvbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import/install Gradio\n",
        "try:\n",
        "  import gradio as gr\n",
        "except:\n",
        "  !pip -q install gradio\n",
        "  import gradio as gr\n",
        "print(f\"Gradio version: {gr.__version__}\")"
      ],
      "metadata": {
        "id": "0fZgFApPSV46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Gradio overview\n",
        "\n",
        "Gradio helps you create macine learning demos and provides a very helpful `Interface` class to easily create an inputs -> model/function -> outputs workflow where the inputs and outputs could be almost anything you want.\n",
        "\n"
      ],
      "metadata": {
        "id": "bKhSkWuySrXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Creating a function to map our inputs and outputs"
      ],
      "metadata": {
        "id": "CfZtkzC-THb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put EffNetB2 on CPU\n",
        "effnetb2.to(\"cpu\")\n",
        "\n",
        "# Check the device\n",
        "next(iter(effnetb2.parameters())).device"
      ],
      "metadata": {
        "id": "ECCrB4bpTpuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, Dict\n",
        "\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
        "    \"\"\"\n",
        "    # Start the timer\n",
        "    start_time = timer()\n",
        "\n",
        "    # Transform the target image and add a batch dimension\n",
        "    img = effnetb2_transforms(img).unsqueeze(0)\n",
        "\n",
        "    # Put model into evaluation mode and turn on inference mode\n",
        "    effnetb2.eval()\n",
        "    with torch.inference_mode():\n",
        "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
        "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
        "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "    # Calculate the prediction time\n",
        "    pred_time = round(timer() - start_time, 5)\n",
        "\n",
        "    # Return the prediction dictionary and prediction time\n",
        "    return pred_labels_and_probs, pred_time"
      ],
      "metadata": {
        "id": "5FpaGDuoTqWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# Get a list of all test image filepaths\n",
        "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "\n",
        "# Randomly select a test image path\n",
        "random_image_path = random.sample(test_data_paths, k=1)[0]\n",
        "\n",
        "# Open the target image\n",
        "image = Image.open(random_image_path)\n",
        "print(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")\n",
        "\n",
        "# Predict on the target image and print out the outputs\n",
        "pred_dict, pred_time = predict(img=image)\n",
        "print(f\"Prediction label and probability dictionary: \\n{pred_dict}\")\n",
        "print(f\"Prediction time: {pred_time} seconds\")"
      ],
      "metadata": {
        "id": "vP4Qq5xhVYM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Creating a list of example images"
      ],
      "metadata": {
        "id": "5pYlZYA5Vxqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our `predict()` function enables us to go from inputs -> transform -> ML model -> outputs.\n",
        "\n",
        "Which is exactly what we need for our Graido demo.\n",
        "\n",
        "But before we create the demo, let's create one more thing: a list of examples.\n",
        "\n",
        "Gradio's `Interface` class takes a list of examples of as an optional parameter `(gradio.Interface(examples=List[Any])`).\n",
        "\n",
        "And the format for the examples parameter is a list of lists.\n",
        "\n",
        "So let's create a list of lists containing random filepaths to our test images.\n",
        "\n",
        "Three examples should be enough."
      ],
      "metadata": {
        "id": "FnMehWKXWAhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of example inputs to our Gradio demo\n",
        "example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\n",
        "example_list"
      ],
      "metadata": {
        "id": "pZz74CFPoeDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4 Building a Gradio interface\n",
        "\n",
        "Let's create a Gradio interface to replicate the workflow:\n",
        "\n",
        "`input: image -> transform -> predict with EffNetB2 -> output: pred, pred prob, time taken`\n",
        "\n",
        "We can do with the `gradio.Interface()` class with the following parameters:\n",
        "\n",
        "*`fn` - a Python function to map `inputs` to `outputs`, in our case, we'll use our `predict()` function.\n",
        "* `inputs` - the input to our interface, such as an image using `gradio.Image()` or `\"image\"`.\n",
        "* `outputs` - the output of our interface once the `inputs` have gone through the `fn`, such as a label using `gradio.Label()` (for our model's predicted labels) or number using `gradio.Number()` (for our model's prediction time).\n",
        "  * Note: Gradio comes with many in-built `inputs` and `outputs` options known as \"`Components`\".\n",
        "* `examples` - a list of examples to showcase for the demo.\n",
        "* `title` - a string title of the demo.\n",
        "* `description` - a string description of the demo.\n",
        "* `article` - a reference note at the bottom of the demo.\n",
        "\n",
        "Once we've created our demo instance of `gr.Interface()`, we can bring it to life using `gradio.Interface()`.`launch()` or `demo.launch()` command."
      ],
      "metadata": {
        "id": "zWGx3m8jo2OH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create title, description and article strings\n",
        "title = \"FoodVision Mini \"\n",
        "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\n",
        "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
        "\n",
        "# Create the Gradio demo\n",
        "demo = gr.Interface(fn=predict, # mapping function from input to output\n",
        "                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n",
        "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n",
        "                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n",
        "                    examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article)\n",
        "\n",
        "# Launch the demo!\n",
        "demo.launch(debug=False, # print errors locally?\n",
        "            share=True) # generate a publically shareable URL?"
      ],
      "metadata": {
        "id": "3xjKIeKTo7Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FoodVision Mini has officially come to life in an interface someone could use and try out.\n",
        "\n",
        "If you set the parameter `share=True` in the `launch()` method, Gradio also provides you with a shareable link such as `https://123XYZ.gradio.app` (this link is an example only and likely expired) which is valid for 72-hours.\n",
        "\n",
        "For more permanent hosting, you can upload your Gradio app to Hugging Face Spaces or anywhere that runs Python code."
      ],
      "metadata": {
        "id": "WvA9Ivi4sY-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Turning our FoodVision Mini Gradio Demo into a deployable app\n",
        "\n",
        "To make our FoodVision Mini demo more permanent, we can package it into an app and upload it to Hugging Face Spaces."
      ],
      "metadata": {
        "id": "xUI8nYfJsvZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 What is Hugging Face Spaces?\n",
        "\n",
        "Hugging Face Spaces is a resource that allows you to host and share machine learning apps.\n",
        "\n",
        "Building a demo is one of the best ways to showcase and test what you've done.\n",
        "\n",
        "And Spaces allows you to do just that.\n",
        "\n",
        "You can think of Hugging Face as the GitHub of machine learning."
      ],
      "metadata": {
        "id": "y35CkjiRs1zK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 Deployed Gradio app structure\n",
        "\n",
        "Let's start to put all of our app files into a single directory:\n",
        "\n",
        "```\n",
        "Colab -> folder with all Gradio files -> upload app files to Hugging Face Spaces -> deploy\n",
        "```\n",
        "For example, our demo might live at the path demos/foodvision_mini/ with the file structure:\n",
        "```\n",
        "demos/\n",
        " foodvision_mini/\n",
        "     09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n",
        "     app.py\n",
        "     examples/\n",
        "        example_1.jpg\n",
        "        example_2.jpg\n",
        "        example_3.jpg\n",
        "     model.py\n",
        "     requirements.txt\n",
        "```"
      ],
      "metadata": {
        "id": "wF-h75y7vDuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3 Creating a `demos` folder to store our FoodVision Mini app files\n",
        "\n",
        "We can do with Python's `pathlib.Path(\"path_to_dir\")` to establish the directory path and `pathlib.Path(\"path_to_dir\").mkdir()` to create it."
      ],
      "metadata": {
        "id": "fzAHClpmvSgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Create FoodVision mini demo path\n",
        "foodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")\n",
        "\n",
        "# Remove files that might already exist there and create new directory\n",
        "if foodvision_mini_demo_path.exists():\n",
        "    shutil.rmtree(foodvision_mini_demo_path)\n",
        "# If the file doesn't exist, create it anyway\n",
        "foodvision_mini_demo_path.mkdir(parents=True,\n",
        "                                exist_ok=True)\n",
        "\n",
        "# Check what's in the folder\n",
        "!ls demos/foodvision_mini/"
      ],
      "metadata": {
        "id": "y6NkQczex-Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.4 Creating a folder of example images to use with our FoodVision Mini demo\n",
        "\n",
        "Now we've got a directory to store our FoodVision Mini demo files, let's add some examples to it.\n",
        "\n",
        "Three example images from the test dataset should be enough.\n",
        "\n",
        "To do so we'll:\n",
        "\n",
        "1. Create an `examples/` directory within the `demos/foodvision_mini` directory.\n",
        "2. Choose three random images from the test dataset and collect their filepaths in a list.\n",
        "3. Copy the three random images from the test dataset to the `demos/foodvision_mini/examples/` directory."
      ],
      "metadata": {
        "id": "GavG4o_2dwbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Create an examples directory\n",
        "foodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\"\n",
        "foodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Collect three random test dataset image paths\n",
        "foodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),\n",
        "                            Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),\n",
        "                            Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n",
        "\n",
        "# 3. Copy the three random images to the examples directory\n",
        "for example in foodvision_mini_examples:\n",
        "    destination = foodvision_mini_examples_path / example.name\n",
        "    print(f\"[INFO] Copying {example} to {destination}\")\n",
        "    shutil.copy2(src=example, dst=destination)"
      ],
      "metadata": {
        "id": "ktnEJURMeM0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to verify our examples are present, let's list the contents of our `demos/foodvision_mini/examples/` directory with `os.listdir()` and then format the filepaths into a list of lists (so it's compatible with Gradio's `gradio.Interface()` `example` parameter)."
      ],
      "metadata": {
        "id": "Xnp42SrggToF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Get example filepaths in a list of lists\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\n",
        "example_list"
      ],
      "metadata": {
        "id": "G68p8QtqgtPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5 Moving our trained EffNetB2 model to our FoodVision Mini demo directory\n",
        "\n",
        "We previously saved our FoodVision Mini EffNetB2 feature extractor model under `models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth`.\n",
        "\n",
        "And rather double up on saved model files, let's move our model to our `demos/foodvision_mini` directory.\n",
        "\n",
        "We can do so using Python's `shutil.move()` method and passing in `src` (the source path of the target file) and `dst` (the destination path of the target file to be moved to) parameters.\n"
      ],
      "metadata": {
        "id": "LaR5EEJ6hFhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Create a source path for our target model\n",
        "effnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n",
        "\n",
        "# Create a destination path for our target model\n",
        "effnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n",
        "\n",
        "# Try to move the file\n",
        "try:\n",
        "    print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")\n",
        "\n",
        "    # Move the model\n",
        "    shutil.move(src=effnetb2_foodvision_mini_model_path,\n",
        "                dst=effnetb2_foodvision_mini_model_destination)\n",
        "\n",
        "    print(f\"[INFO] Model move complete.\")\n",
        "\n",
        "# If the model has already been moved, check if it exists\n",
        "except:\n",
        "    print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")\n",
        "    print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\")"
      ],
      "metadata": {
        "id": "pGEknOrMhyJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.6 Turning our EffNetB2 model into a Python script (`model.py`)\n",
        "\n"
      ],
      "metadata": {
        "id": "uU7H88CQiL7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_mini/model.py\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def create_effnetb2_model(num_classes:int=3,\n",
        "                          seed:int=42):\n",
        "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int, optional): number of classes in the classifier head.\n",
        "            Defaults to 3.\n",
        "        seed (int, optional): random seed value. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): EffNetB2 feature extractor model.\n",
        "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
        "    \"\"\"\n",
        "    # Create EffNetB2 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "    # Freeze all layers in base model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Change classifier head with random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.3, inplace=True),\n",
        "        nn.Linear(in_features=1408, out_features=num_classes),\n",
        "    )\n",
        "\n",
        "    return model, transforms"
      ],
      "metadata": {
        "id": "C9Jwpcexi5ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.7 Turning our FoodVision Mini Gradio app into a Pythhon script (`app.py`)"
      ],
      "metadata": {
        "id": "BLiFOf-DjNIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_mini/app.py\n",
        "### 1. Imports and class names setup ###\n",
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from model import create_effnetb2_model\n",
        "from timeit import default_timer as timer\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Setup class names\n",
        "class_names = [\"pizza\", \"steak\", \"sushi\"]\n",
        "\n",
        "### 2. Model and transforms preparation ###\n",
        "\n",
        "# Create EffNetB2 model\n",
        "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
        "    num_classes=3, # len(class_names) would also work\n",
        ")\n",
        "\n",
        "# Load saved weights\n",
        "effnetb2.load_state_dict(\n",
        "    torch.load(\n",
        "        f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",\n",
        "        map_location=torch.device(\"cpu\"),  # load to CPU\n",
        "    )\n",
        ")\n",
        "\n",
        "### 3. Predict function ###\n",
        "\n",
        "# Create predict function\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
        "    \"\"\"\n",
        "    # Start the timer\n",
        "    start_time = timer()\n",
        "\n",
        "    # Transform the target image and add a batch dimension\n",
        "    img = effnetb2_transforms(img).unsqueeze(0)\n",
        "\n",
        "    # Put model into evaluation mode and turn on inference mode\n",
        "    effnetb2.eval()\n",
        "    with torch.inference_mode():\n",
        "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
        "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
        "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "    # Calculate the prediction time\n",
        "    pred_time = round(timer() - start_time, 5)\n",
        "\n",
        "    # Return the prediction dictionary and prediction time\n",
        "    return pred_labels_and_probs, pred_time\n",
        "\n",
        "### 4. Gradio app ###\n",
        "\n",
        "# Create title, description and article strings\n",
        "title = \"FoodVision Mini \"\n",
        "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\n",
        "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
        "\n",
        "# Create examples list from \"examples/\" directory\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
        "\n",
        "# Create the Gradio demo\n",
        "demo = gr.Interface(fn=predict, # mapping function from input to output\n",
        "                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n",
        "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n",
        "                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n",
        "                    # Create examples list from \"examples/\" directory\n",
        "                    examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article)\n",
        "\n",
        "# Launch the demo!\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "DtKiitn6jYYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.8 Creating a requirements file for FoodVision Mini (`requirements.txt`)"
      ],
      "metadata": {
        "id": "B3_ovOCRj9im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_mini/requirements.txt\n",
        "torch\n",
        "torchvision\n",
        "gradio==3.1.4"
      ],
      "metadata": {
        "id": "psy3BzSYknKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Deploying our FoodVision Mini app to HuggingFace Spaces\n"
      ],
      "metadata": {
        "id": "b2k2Lau7knyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1 Downloading our FoodVision Mini app files\n",
        "\n",
        "To begin uploading our files to Hugging Face, let's now download them from Google Colab (or wherever you're running this notebook).\n",
        "\n",
        "To do so, we'll first compress the files into a single zip folder via the command:\n",
        "\n",
        "`zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"`\n",
        "\n",
        "Where:\n",
        "\n",
        "* `zip` stands for \"zip\" as in \"please zip together the files in the following directory\".\n",
        "* `-r` stands for \"recursive\" as in, \"go through all of the files in the target directory\".\n",
        "* `../foodvision_mini.zip` is the target directory we'd like our files to be zipped to.\n",
        "* `*` stands for \"all the files in the current directory\".\n",
        "* `-x` stands for \"exclude these files\".\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wKwliZ4tCBxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls demos/foodvision_mini"
      ],
      "metadata": {
        "id": "J5P4jbcWCLGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change into and then zip the foodvision_mini folder but exclude certain files\n",
        "!cd demos/foodvision_mini && zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
        "\n",
        "# Download the zipped FoodVision Mini app (if running in Google Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(\"demos/foodvision_mini.zip\")\n",
        "except:\n",
        "    print(\"Not running in Google Colab, can't use google.colab.files.download(), please manually download.\")"
      ],
      "metadata": {
        "id": "3gLz9My5CQOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2 Running our FoodVision Mini demo locally\n",
        "\n",
        "If you download the foodvision_mini.zip file, you can test it locally by:\n",
        "\n",
        "1. Unzipping the file.\n",
        "2. Opening terminal or a command line prompt.\n",
        "3. Changing into the `foodvision_mini` directory (`cd foodvision_mini`).\n",
        "4. Creating an environment (`python3 -m venv env`).\n",
        "5. Activating the environment (`source env/bin/activate`).\n",
        "6. Installing the requirements (`pip install -r requirements.txt`, the \"`-r`\" is for recursive).\n",
        "  * Note: This step may take 5-10 minutes depending on your internet connection. And if you're facing errors, you may need to upgrade pip first: `pip install --upgrade pip`.\n",
        "7. Run the app (`python3 app.py`)."
      ],
      "metadata": {
        "id": "Xd9UqyV9Doad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3 Uploading to Hugging Face"
      ],
      "metadata": {
        "id": "WoAM7UWHD62o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Creating FoodVision Big"
      ],
      "metadata": {
        "id": "UVzai-tbGNJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.1 Creating a model and transforms for FoodVision Big"
      ],
      "metadata": {
        "id": "4g-i-kFVGY00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create EffNetB2 model capable of fitting to 101 classes for Food101\n",
        "effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)"
      ],
      "metadata": {
        "id": "XuyfHj2eGlzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# # Get a summary of EffNetB2 feature extractor for Food101 with 101 output classes (uncomment for full output)\n",
        "# summary(effnetb2_food101,\n",
        "#         input_size=(1, 3, 224, 224),\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "wE7-0E1pGvjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Food101 training data transforms (only perform data augmentation on the training images)\n",
        "food101_train_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.TrivialAugmentWide(),\n",
        "    effnetb2_transforms,\n",
        "])"
      ],
      "metadata": {
        "id": "VHEqvSFSGzAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training transforms:\\n{food101_train_transforms}\\n\")\n",
        "print(f\"Testing transforms:\\n{effnetb2_transforms}\")"
      ],
      "metadata": {
        "id": "GtDd7uQrHYiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.2 Getting data for FoodVision Big"
      ],
      "metadata": {
        "id": "StTyXcMlHall"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "# Setup data directory\n",
        "from pathlib import Path\n",
        "data_dir = Path(\"data\")\n",
        "\n",
        "# Get training data (~750 images x 101 food classes)\n",
        "train_data = datasets.Food101(root=data_dir, # path to download data to\n",
        "                              split=\"train\", # dataset split to get\n",
        "                              transform=food101_train_transforms, # perform data augmentation on training data\n",
        "                              download=True) # want to download?\n",
        "\n",
        "# Get testing data (~250 images x 101 food classes)\n",
        "test_data = datasets.Food101(root=data_dir,\n",
        "                             split=\"test\",\n",
        "                             transform=effnetb2_transforms, # perform normal EffNetB2 transforms on test data\n",
        "                             download=True)"
      ],
      "metadata": {
        "id": "0z7ZmAfNHhMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Food101 class names\n",
        "food101_class_names = train_data.classes\n",
        "\n",
        "# View the first 10\n",
        "food101_class_names[:10]"
      ],
      "metadata": {
        "id": "XzHWXwdHJS7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3 Creating a subset of the Food101 dataset for faster experimenting\n",
        "\n",
        "This is optional.\n",
        "\n",
        "We don't need to create another subset of the Food101 dataset, we could train and evaluate a model across the whole 101,000 images.\n",
        "\n",
        "But to keep training fast, let's create a 20% split of the training and test datasets.\n",
        "\n",
        "Our goal will be to see if we can beat the original Food101 paper's best results with only 20% of the data."
      ],
      "metadata": {
        "id": "cKT2onRXJWn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(dataset:torchvision.datasets, split_size:float=0.2, seed:int=42):\n",
        "    \"\"\"Randomly splits a given dataset into two proportions based on split_size and seed.\n",
        "\n",
        "    Args:\n",
        "        dataset (torchvision.datasets): A PyTorch Dataset, typically one from torchvision.datasets.\n",
        "        split_size (float, optional): How much of the dataset should be split?\n",
        "            E.g. split_size=0.2 means there will be a 20% split and an 80% split. Defaults to 0.2.\n",
        "        seed (int, optional): Seed for random generator. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (random_split_1, random_split_2) where random_split_1 is of size split_size*len(dataset) and\n",
        "            random_split_2 is of size (1-split_size)*len(dataset).\n",
        "    \"\"\"\n",
        "    # Create split lengths based on original dataset length\n",
        "    length_1 = int(len(dataset) * split_size) # desired length\n",
        "    length_2 = len(dataset) - length_1 # remaining length\n",
        "\n",
        "    # Print out info\n",
        "    print(f\"[INFO] Splitting dataset of length {len(dataset)} into splits of size: {length_1} ({int(split_size*100)}%), {length_2} ({int((1-split_size)*100)}%)\")\n",
        "\n",
        "    # Create splits with given random seed\n",
        "    random_split_1, random_split_2 = torch.utils.data.random_split(dataset,\n",
        "                                                                   lengths=[length_1, length_2],\n",
        "                                                                   generator=torch.manual_seed(seed)) # set the random seed for reproducible splits\n",
        "    return random_split_1, random_split_2"
      ],
      "metadata": {
        "id": "IHneG9nkJcth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training 20% split of Food101\n",
        "train_data_food101_20_percent, _ = split_dataset(dataset=train_data,\n",
        "                                                 split_size=0.2)\n",
        "\n",
        "# Create testing 20% split of Food101\n",
        "test_data_food101_20_percent, _ = split_dataset(dataset=test_data,\n",
        "                                                split_size=0.2)\n",
        "\n",
        "len(train_data_food101_20_percent), len(test_data_food101_20_percent)"
      ],
      "metadata": {
        "id": "Fe2GXS5KJulo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.4 Turning our Food101 datasets into `DataLoader`s"
      ],
      "metadata": {
        "id": "1MO1ceUcJxop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2 if os.cpu_count() <= 4 else 4 # this value is very experimental and will depend on the hardware you have available, Google Colab generally provides 2x CPUs\n",
        "\n",
        "# Create Food101 20 percent training DataLoader\n",
        "train_dataloader_food101_20_percent = torch.utils.data.DataLoader(train_data_food101_20_percent,\n",
        "                                                                  batch_size=BATCH_SIZE,\n",
        "                                                                  shuffle=True,\n",
        "                                                                  num_workers=NUM_WORKERS)\n",
        "# Create Food101 20 percent testing DataLoader\n",
        "test_dataloader_food101_20_percent = torch.utils.data.DataLoader(test_data_food101_20_percent,\n",
        "                                                                 batch_size=BATCH_SIZE,\n",
        "                                                                 shuffle=False,\n",
        "                                                                 num_workers=NUM_WORKERS)"
      ],
      "metadata": {
        "id": "MCPX1D0wJ8Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.5 Training FoodVision Big model"
      ],
      "metadata": {
        "id": "bkhyj8r9KLxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.Adam(params=effnetb2_food101.parameters(),\n",
        "                             lr=1e-3)\n",
        "\n",
        "# Setup loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1) # throw in a little label smoothing because so many classes\n",
        "\n",
        "# Want to beat original Food101 paper with 20% of data, need 56.4%+ acc on test dataset\n",
        "set_seeds()\n",
        "effnetb2_food101_results = engine.train(model=effnetb2_food101,\n",
        "                                        train_dataloader=train_dataloader_food101_20_percent,\n",
        "                                        test_dataloader=test_dataloader_food101_20_percent,\n",
        "                                        optimizer=optimizer,\n",
        "                                        loss_fn=loss_fn,\n",
        "                                        epochs=5,\n",
        "                                        device=device)"
      ],
      "metadata": {
        "id": "LU-uPEZkKVEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.6 Inspecting loss curves of FoodVision Big model"
      ],
      "metadata": {
        "id": "3yMdFyZqL79q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "# Check out the loss curves for FoodVision Big\n",
        "plot_loss_curves(effnetb2_food101_results)"
      ],
      "metadata": {
        "id": "pjNcAQj1MA41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.7 Saving and loading FoodVision Big"
      ],
      "metadata": {
        "id": "8yPcw7tHMZcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import utils\n",
        "\n",
        "# Create a model path\n",
        "effnetb2_food101_model_path = \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"\n",
        "\n",
        "# Save FoodVision Big model\n",
        "utils.save_model(model=effnetb2_food101,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=effnetb2_food101_model_path)"
      ],
      "metadata": {
        "id": "KOvROubxMd19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Food101 compatible EffNetB2 instance\n",
        "loaded_effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n",
        "\n",
        "# Load the saved model's state_dict()\n",
        "loaded_effnetb2_food101.load_state_dict(torch.load(\"models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"))"
      ],
      "metadata": {
        "id": "NxWK_yhpMq8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.8 Checking FoodVision Big size"
      ],
      "metadata": {
        "id": "Om9l8-ozMsow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_effnetb2_food101_model_size = Path(\"models\", effnetb2_food101_model_path).stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\n",
        "print(f\"Pretrained EffNetB2 feature extractor Food101 model size: {pretrained_effnetb2_food101_model_size} MB\")"
      ],
      "metadata": {
        "id": "EQZnBraCMv4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Turning our FoodVision Big model into a deployable app\n",
        "\n",
        "We've got a trained and saved EffNetB2 model on 20% of the Food101 dataset.\n",
        "\n",
        "And instead of letting our model live in a folder all its life, let's deploy it!\n",
        "\n",
        "We'll deploy our FoodVision Big model in the same way we deployed our FoodVision Mini model, as a Gradio demo on Hugging Face Spaces.\n",
        "\n",
        "To begin, let's create a `demos/foodvision_big/` directory to store our FoodVision Big demo files as well as a demos/foodvision_big/examples directory to hold an example image to test the demo with.\n",
        "\n",
        "When we're finished we'll have the following file structure:\n",
        "\n",
        "```\n",
        "demos/\n",
        "  foodvision_big/\n",
        "    09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\n",
        "    app.py\n",
        "    class_names.txt\n",
        "    examples/\n",
        "      example_1.jpg\n",
        "    model.py\n",
        "    requirements.txt\n",
        "```\n"
      ],
      "metadata": {
        "id": "d1G0pb6-M83C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where:\n",
        "\n",
        "* 09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth is our trained PyTorch model file.\n",
        "* app.py contains our FoodVision Big Gradio app.\n",
        "* class_names.txt contains all of the class names for FoodVision Big.\n",
        "* examples/ contains example images to use with our Gradio app.\n",
        "* model.py contains the model definition as well as any transforms associated with the model.\n",
        "* requirements.txt contains the dependencies to run our app such as torch, torchvision and gradio."
      ],
      "metadata": {
        "id": "PoH9o4LJNBV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create FoodVision Big demo path\n",
        "foodvision_big_demo_path = Path(\"demos/foodvision_big/\")\n",
        "\n",
        "# Make FoodVision Big demo directory\n",
        "foodvision_big_demo_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Make FoodVision Big demo examples directory\n",
        "(foodvision_big_demo_path / \"examples\").mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "qq-uK6rg7GkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.1 Downloading an example image and moving it to the `examples` directory"
      ],
      "metadata": {
        "id": "OEuuKmKm7K2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and move an example image\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\n",
        "!mv 04-pizza-dad.jpeg demos/foodvision_big/examples/04-pizza-dad.jpg\n",
        "\n",
        "# Move trained model to FoodVision Big demo folder (will error if model is already moved)\n",
        "!mv models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth demos/foodvision_big"
      ],
      "metadata": {
        "id": "PkRndIAE7Swh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.2 Saving Food101 class names to file (`class_names.txt`)"
      ],
      "metadata": {
        "id": "gDfqZX6U7Wvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the first 10 Food101 class names\n",
        "food101_class_names[:10]"
      ],
      "metadata": {
        "id": "oRLBgtTW7lff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create path to Food101 class names\n",
        "foodvision_big_class_names_path = foodvision_big_demo_path / \"class_names.txt\"\n",
        "\n",
        "# Write Food101 class names list to file\n",
        "with open(foodvision_big_class_names_path, \"w\") as f:\n",
        "    print(f\"[INFO] Saving Food101 class names to {foodvision_big_class_names_path}\")\n",
        "    f.write(\"\\n\".join(food101_class_names)) # leave a new line between each class"
      ],
      "metadata": {
        "id": "Un1IqK2H7nOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open Food101 class names file and read each line into a list\n",
        "with open(foodvision_big_class_names_path, \"r\") as f:\n",
        "    food101_class_names_loaded = [food.strip() for food in  f.readlines()]\n",
        "\n",
        "# View the first 5 class names loaded back in\n",
        "food101_class_names_loaded[:5]"
      ],
      "metadata": {
        "id": "_yZPFP4D7s4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.3 Turning our FoodVision Big model into a Python script (`model.py`)"
      ],
      "metadata": {
        "id": "LEaf7CBe7372"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_big/model.py\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def create_effnetb2_model(num_classes:int=3,\n",
        "                          seed:int=42):\n",
        "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int, optional): number of classes in the classifier head.\n",
        "            Defaults to 3.\n",
        "        seed (int, optional): random seed value. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): EffNetB2 feature extractor model.\n",
        "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
        "    \"\"\"\n",
        "    # Create EffNetB2 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "    # Freeze all layers in base model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Change classifier head with random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.3, inplace=True),\n",
        "        nn.Linear(in_features=1408, out_features=num_classes),\n",
        "    )\n",
        "\n",
        "    return model, transforms"
      ],
      "metadata": {
        "id": "G3TU52HX8OtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.4 Turning our FoodVision Big Gradio app into a Python script (`app.py`)"
      ],
      "metadata": {
        "id": "o142gfJq8Zq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_big/app.py\n",
        "### 1. Imports and class names setup ###\n",
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from model import create_effnetb2_model\n",
        "from timeit import default_timer as timer\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Setup class names\n",
        "with open(\"class_names.txt\", \"r\") as f: # reading them in from class_names.txt\n",
        "    class_names = [food_name.strip() for food_name in  f.readlines()]\n",
        "\n",
        "### 2. Model and transforms preparation ###\n",
        "\n",
        "# Create model\n",
        "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
        "    num_classes=101, # could also use len(class_names)\n",
        ")\n",
        "\n",
        "# Load saved weights\n",
        "effnetb2.load_state_dict(\n",
        "    torch.load(\n",
        "        f=\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\",\n",
        "        map_location=torch.device(\"cpu\"),  # load to CPU\n",
        "    )\n",
        ")\n",
        "\n",
        "### 3. Predict function ###\n",
        "\n",
        "# Create predict function\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
        "    \"\"\"\n",
        "    # Start the timer\n",
        "    start_time = timer()\n",
        "\n",
        "    # Transform the target image and add a batch dimension\n",
        "    img = effnetb2_transforms(img).unsqueeze(0)\n",
        "\n",
        "    # Put model into evaluation mode and turn on inference mode\n",
        "    effnetb2.eval()\n",
        "    with torch.inference_mode():\n",
        "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
        "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
        "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "    # Calculate the prediction time\n",
        "    pred_time = round(timer() - start_time, 5)\n",
        "\n",
        "    # Return the prediction dictionary and prediction time\n",
        "    return pred_labels_and_probs, pred_time\n",
        "\n",
        "### 4. Gradio app ###\n",
        "\n",
        "# Create title, description and article strings\n",
        "title = \"FoodVision Big \"\n",
        "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food into [101 different classes](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names.txt).\"\n",
        "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
        "\n",
        "# Create examples list from \"examples/\" directory\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=[\n",
        "        gr.Label(num_top_classes=5, label=\"Predictions\"),\n",
        "        gr.Number(label=\"Prediction time (s)\"),\n",
        "    ],\n",
        "    examples=example_list,\n",
        "    title=title,\n",
        "    description=description,\n",
        "    article=article,\n",
        ")\n",
        "\n",
        "# Launch the app!\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "z1Ix2pgN8jA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.5 Creating requirements file for FoodVision Big (`requirements.txt`)"
      ],
      "metadata": {
        "id": "1jzupv2T8teq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_big/requirements.txt\n",
        "torch\n",
        "torchvision\n",
        "gradio==3.1.4"
      ],
      "metadata": {
        "id": "Rex_Cmno82Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.6 Downloading our FoodVision Big app files"
      ],
      "metadata": {
        "id": "aUgRUT1m847R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip foodvision_big folder but exclude certain files\n",
        "!cd demos/foodvision_big && zip -r ../foodvision_big.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
        "\n",
        "# Download the zipped FoodVision Big app (if running in Google Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(\"demos/foodvision_big.zip\")\n",
        "except:\n",
        "    print(\"Not running in Google Colab, can't use google.colab.files.download()\")"
      ],
      "metadata": {
        "id": "7o0V6rol9Omc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.7 Deploying our FoodVision app to Huggingface Spaces\n",
        "\n",
        "App can be viewwed here:\n",
        "\n",
        "https://huggingface.co/spaces/chpham92/foodvision_big"
      ],
      "metadata": {
        "id": "Q4C3-Y3b9UR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IPython is a library to help work with Python interactively\n",
        "from IPython.display import IFrame\n",
        "\n",
        "# Embed FoodVision Big Gradio demo as an iFrame\n",
        "IFrame(src=\"https://huggingface.co/spaces/chpham92/foodvision_big\", width=900, height=750)"
      ],
      "metadata": {
        "id": "dRIpgiD99ax4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}