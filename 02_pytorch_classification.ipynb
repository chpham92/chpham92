{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOOxIVgiicWh2e/2tyEMLIL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chpham92/chpham92/blob/main/02_pytorch_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02. Neural network classification with PyTorch\n",
        "Classification is a probelm of predicting whether something is one thing or another (there can be multiple things as the options)\n",
        "\n",
        "Book version of this notebook - https://learnpytorch.io/02_pytorch_classification/\n",
        "All other resources - https://github.com/mrdbourke/pytorch-deep-learning\n",
        "Stuck? Ask a question here"
      ],
      "metadata": {
        "id": "0h7Vs0BZGMs8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Make classification data and get it ready\n"
      ],
      "metadata": {
        "id": "zmT5i2uEGn7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.datasets import make_circles"
      ],
      "metadata": {
        "id": "JRXbgJOQHN5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make 1000 samples\n",
        "n_samples = 1000\n",
        "\n",
        "# Create make_circles\n",
        "X, y = make_circles(n_samples,\n",
        "                    noise=0.03,\n",
        "                    random_state=42)"
      ],
      "metadata": {
        "id": "FrW81AqyHUFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X), len(y)"
      ],
      "metadata": {
        "id": "QNmUdAieHpI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"First 5 samples of X:/n {X[:5]}\")\n",
        "print(f\"First 5 samples of y:/n {y[:5]}\")\n"
      ],
      "metadata": {
        "id": "RHJjme0AHvcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a DataFrame of circle data\n",
        "import pandas as pd\n",
        "circles = pd.DataFrame({\"X1\": X[:,0],\n",
        "                       \"X2\": X[:,1],\n",
        "                       \"label\": y})\n",
        "circles.head(10)"
      ],
      "metadata": {
        "id": "9R5j0PpQH_8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Visualize Visualize\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x=X[:,0],\n",
        "           y=X[:,1],\n",
        "           c=y,\n",
        "           cmap=plt.cm.RdYlBu);"
      ],
      "metadata": {
        "id": "35h3YSzgIkys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The data we're working with is often referred to as a toy dataset, a dataset that is small enough to experiment but still sizeable enough to practice the fundamentals"
      ],
      "metadata": {
        "id": "I8hYsp32JQqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Check input and output shapes"
      ],
      "metadata": {
        "id": "xe-fd-4mJG9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "TMT88aW7J3r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vuew the first example of features and labels\n",
        "X_sample = X[0]\n",
        "y_sample = y[0]\n",
        "\n",
        "print(f\"Values for one example of X: {X_sample} and the same for y: {y_sample}\")\n",
        "print(f\"Shapes for one example of X: {X_sample.shape} and the same for y: {y_sample.shape}\")"
      ],
      "metadata": {
        "id": "HKjSSrPTJ9bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Turn data into tensors and create train and test splits"
      ],
      "metadata": {
        "id": "ClQNYItiKt59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "R1S9dsiHLJmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X)"
      ],
      "metadata": {
        "id": "3CBVp7opLVpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn data into tensors\n",
        "X = torch.from_numpy(X).type(torch.float)\n",
        "y = torch.from_numpy(y).type(torch.float)\n",
        "\n",
        "X[:5], y[:5]"
      ],
      "metadata": {
        "id": "eNjHuVSkLNEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X), X.dtype, y.dtype"
      ],
      "metadata": {
        "id": "82PMoPPpLvWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "FbQWpgn5L3XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train), len(X_test), len(y_train), len(y_test)"
      ],
      "metadata": {
        "id": "wnbdTtmzMo3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Building a model\n",
        "\n",
        "Let's build a model to classify our blue adn red dots\n",
        "\n",
        "To do so, we want to:\n",
        "1. Setup device agnostic code so our code wil run on an accelerator (GPU) if there is one\n",
        "2. Consttruct a model (by subclassing 'nn.Module')\n",
        "3. Define a loss function and optimizer\n",
        "4. Create a training and test loop"
      ],
      "metadata": {
        "id": "4FwT-Ge0MsYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyTorch and nn\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Make device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "device"
      ],
      "metadata": {
        "id": "Sa4rcVcOzqDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've setup device agnostic code, let's create a model that:\n",
        "\n",
        "1. Subclasses 'nn.Module' (almost all models in PyTorch subclass 'nn.Module')\n",
        "2. Create 2 'nn.Linear()' layers that are capable of. handling the shape of our data\n",
        "3. Defines a 'forwrad()' method that outlines the forward pass (or forward computation) of the model\n",
        "4. Instantiate an instance of our model class and send it to the 'device'"
      ],
      "metadata": {
        "id": "uv7BIo6xz9Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Construct a model that subclasses nn.Module\n",
        "class CircleModelV0(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      # 2. Create 2 nn.Linear layers capable of handling the shapes our our data\n",
        "      self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 fetures and upscales to 5 features\n",
        "      self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features from previous layer and outputs a single feature (same shape as y)\n",
        "\n",
        "\n",
        "      # 3. Define a forward() method that outlines the forward pass\n",
        "    def forward(self, x):\n",
        "        return self.layer_2(self.layer_1(x)) # x -> layer 1 -> layer 2 -> output\n",
        "\n",
        "# 4. Instantiate an instance of our model class and send it to the target device\n",
        "model_0 = CircleModelV0().to(device)\n",
        "model_0"
      ],
      "metadata": {
        "id": "EfGO4xIS01aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's replicate the model above using nn.Sequential()\n",
        "model_0 = nn.Sequential(\n",
        "    nn.Linear(in_features=2, out_features=5),\n",
        "    nn.Linear(in_features=5, out_features=1)\n",
        ").to(device)\n",
        "\n",
        "model_0"
      ],
      "metadata": {
        "id": "WN5bMw1Y2YCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "id": "5fVQHUqA6GQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "with torch.inference_mode():\n",
        "  untrained_preds = model_0(X_test.to(device))\n",
        "print(f\"Length of predictions: {len(untrained_preds)}, Shape of predictions: {untrained_preds.shape}\")\n",
        "print(f\"Length of test samples: {len(X_test)}, Shape: {X_test.shape}\")\n",
        "print(f\"\\nFirst 10 Predictions:\\n {untrained_preds[:10]}\")\n",
        "print(f\"\\nFirst 10 labels:\\n {y_test[:10]}\")"
      ],
      "metadata": {
        "id": "hqVpRRVt4yS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[:10], y_test[:10]"
      ],
      "metadata": {
        "id": "eKSC15rE7Z2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Setup loss function and optimizer\n",
        "\n",
        "Which loss function or optimizer should you use?\n",
        "\n",
        "Again this is problem specific\n",
        "\n",
        "For example for regression you might want MAE or MSE (mean absolute error or mean squared error).and\n",
        "\n",
        "For classification you might want binary cross entropy or categorical cross entropy.\n",
        "\n",
        "As a reminder, the loss fucntion measures how \"wrong\" your model's predictions are\n",
        "\n",
        "And for optimizers, two of the most common and useful are SGD and Adam, however PyTorch has many built-in options\n",
        "\n",
        "* For the loss function we're going to use 'torch.nn.BCEWithLogitsLoss()'\n"
      ],
      "metadata": {
        "id": "xyrlTcqr7qO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the loss function\n",
        "# loss_fn = BCELoss() # BCELoss = requires inputs to have gone through the sigmoid activation function prior to inout to BCELoss\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Setup the optimizer\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
        "                            lr=0.1)\n"
      ],
      "metadata": {
        "id": "IUjy3JS7kPMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy - out of 100 examples, what percentage deos our model get right?\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "  correct = torch.eq(y_true, y_pred).sum().item()\n",
        "  acc = (correct / len(y_pred)) * 100\n",
        "  return acc"
      ],
      "metadata": {
        "id": "mZCYVN6AmYTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Train model\n",
        "\n",
        "To train our model, we're going to need to build a training loop\n",
        "\n",
        "1. Forward pass\n",
        "2. Calculate the loss\n",
        "3. Optimizer zero grad\n",
        "4. Loss backward (backpropagation)\n",
        "5. Optimizer step (gradient descent)"
      ],
      "metadata": {
        "id": "Ctu0xyammb_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Going from raw logits -> prediction probabilities -> preddiction labels\n",
        "Our model outputs are going to be raw **logits**\n",
        "\n",
        "we can convert the **logits** into prediction probabilities by passing them to some kind of activation function (e.g sigmoid for binary classification and softmax for multiclass classification).\n",
        "\n",
        "Then we can convert our model's prediction probabilties to **prediction labels** by either rounding them or taking the argmax()"
      ],
      "metadata": {
        "id": "s_2rylgknNU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vuew the first 5 outputs of the forwrad pass on the test data\n",
        "model_0.eval()\n",
        "with torch.inference_mode():\n",
        "  y_logits = model_0(X_test.to(device))\n",
        "y_logits[:5]"
      ],
      "metadata": {
        "id": "pwWwt51an6If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[:5]"
      ],
      "metadata": {
        "id": "_OCXFuMWo1lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the sigmooid activation function on our model logits\n",
        "y_pred_probs = torch.sigmoid(y_logits)\n",
        "y_pred_probs[:5]"
      ],
      "metadata": {
        "id": "P_AsUXL2pNhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our prediction probability values, we need to perform a range-style rounding on them:\n",
        "* 'y_pred_probs' >= 0.5 y=1 (class 1)\n",
        "* y_pred_probs < 0.5 y=0 (class 0)"
      ],
      "metadata": {
        "id": "eJSzHcvHpsHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the predicted labels\n",
        "y_preds = torch.round(y_pred_probs)\n",
        "\n",
        "# In full (logits -> pred prob -> pred labels)\n",
        "y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device)))) # Removed [:5]\n",
        "\n",
        "# Check for equality\n",
        "print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n",
        "\n",
        "# Get rid of extra dimension\n",
        "y_preds.squeeze()"
      ],
      "metadata": {
        "id": "t48aGjuapZDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set teh number of epochs\n",
        "epochs = 1000\n",
        "\n",
        "# Put data to taregt device\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "# Build training and evaluation loop\n",
        "for epoch in range(epochs):\n",
        "  ### Training\n",
        "  model_0.train()\n",
        "\n",
        "  # 1. Forward pass\n",
        "  y_logits = model_0(X_train).squeeze()\n",
        "  y_pred = torch.round(torch.sigmoid(y_logits)) # Turn logits -> pred_probs -> pred_labels\n",
        "\n",
        "  # Calculate loss/accuracy\n",
        "  # loss = loss_fn(torch.sigmoid(y_logits), # nn.BCELoss expects prediction probabilitiesas input\n",
        "  #                y_train)\n",
        "  loss = loss_fn(y_logits, y_train) # nn.BCEWithLogitsLoss expects raw logits as input\n",
        "  acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Loss backward (backpropagation)\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Optimizer step (graident descent)\n",
        "  optimizer.step()\n",
        "\n",
        "  ### Testing\n",
        "  model_0.eval()\n",
        "  with torch.inference_mode():\n",
        "    # 1. Forward pass\n",
        "    test_logits = model_0(X_test).squeeze()\n",
        "    test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "    # 2. Calculate test lodd/acc\n",
        "    test_loss = loss_fn(test_logits, y_test)\n",
        "    test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
        "# Print out what's happening\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "A4YY_lRfpicA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Make predictions and evaluate the model\n",
        "\n",
        "From the metrrics it looks like our model isn't leanirng anything...\n",
        "\n",
        "To inspect it let's maek some predcitons and make them visual\n",
        "\n",
        "To do so, we're going to import a function called 'plot_decision_boundary()'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6kcXNFVTvf1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "# Download helper function from Learn PyTorch repo (if it's not downloaded yet)\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)\n",
        "\n",
        "from helper_functions import plot_predictions, plot_decision_boundary"
      ],
      "metadata": {
        "id": "jV9ApxD0yIEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot decision boundary\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_0, X_train, y_train)\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_0, X_test, y_test)"
      ],
      "metadata": {
        "id": "0y8KRbeRyx5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improving our model (from a model persepctive)\n",
        "\n",
        "* Add more layers - give the model more chances to learn about patterns in the data\n",
        "* Add more hidden units - got from 5 hidden units to 10 hidden units\n",
        "* Fit for longer\n",
        "* Chaning the activation functions\n",
        "* Change the leanring rate\n",
        "* Change the loss function\n",
        "\n",
        "These options are all from a model's perspective because they deal with the model, rather than the data\n",
        "\n",
        "And because these optionsa re all values that we as machine lenring engineers and data scientists can change, they are referred to as **hyperparameters**\n",
        "\n",
        "Let's try and improve our model by:\n",
        "* adding more hidden units: 5 -> 10\n",
        "* Increase the number of layers: 2 -> 3\n",
        "* Increase the number of epochs: 100 -> 1000"
      ],
      "metadata": {
        "id": "m5PmYJ-HzynF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CircleModelV1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
        "    self.layer_2 = nn.Linear(in_features=10, out_features=10)\n",
        "    self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer_3(self.layer_2(self.layer_1(x))) # this way of writing operations leverages speed ups where possible behind the scenes\n",
        "\n",
        "model_1 = CircleModelV1().to(device)\n",
        "model_1"
      ],
      "metadata": {
        "id": "gpHChBlk1O5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a loss function\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Create an optimizer\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(),\n",
        "                            lr=0.1)\n",
        "\n",
        "# write a training and evaluation loop for model_1\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "epochs = 1000\n",
        "\n",
        "# Put data on the target device\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  ### Training\n",
        "  model_1.train()\n",
        "  # 1. Forward pass\n",
        "  y_logits = model_1(X_train).squeeze()\n",
        "  y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> pred probabilities -> prediction labels\n",
        "\n",
        "  # Calculate the loss/acc\n",
        "  loss = loss_fn(y_logits, y_train)\n",
        "  acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Loss backward (backpropagation)\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Optimizer step\n",
        "  optimizer.step()\n",
        "\n",
        "  ### Testing\n",
        "  model_1.eval()\n",
        "  with torch.inference_mode():\n",
        "    # 1. Forward pass\n",
        "    test_logits = model_1(X_test).squeeze()\n",
        "    test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "    # Calculate the loss\n",
        "    test_loss = loss_fn(test_logits, y_test)\n",
        "    test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
        "# Print out what's happening\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "40Ij8aeE0e18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot decision boundary\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_1, X_train, y_train)\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_1, X_test, y_test)"
      ],
      "metadata": {
        "id": "Qp84vAHx9oBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Preparing data to see if our model can fit a straight line\n",
        "\n",
        "One way to troubleshoota larger problem is to test out a smaller problem\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kj3zy0Sn-RxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create some data (same as notebook 01)\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.01\n",
        "\n",
        "# Create data\n",
        "X_regression = torch.arange(start, end, step).unsqueeze(dim=1)\n",
        "y_regression = weight * X_regression + bias\n",
        "\n",
        "# Check the data\n",
        "print(len(X_regression), len(y_regression))\n",
        "print(X_regression[:10], y_regression[:10])"
      ],
      "metadata": {
        "id": "BkS1I3D9-wq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and test splits\n",
        "train_split = int(0.8 * len(X_regression))\n",
        "X_train_regression = X_regression[:train_split]\n",
        "y_train_regression = y_regression[:train_split]\n",
        "X_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]\n",
        "\n",
        "# Check the lengths of each\n",
        "print(len(X_train_regression))\n",
        "print(len(X_test_regression))\n",
        "print(len(y_train_regression))\n",
        "print(len(y_test_regression))"
      ],
      "metadata": {
        "id": "AYiHVXTm_aZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(train_data=X_train_regression,\n",
        "                 train_labels=y_train_regression,\n",
        "                 test_data=X_test_regression,\n",
        "                 test_labels=y_test_regression)"
      ],
      "metadata": {
        "id": "igJ1sQB0BLWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Adjusting model_1 to fit a straight line"
      ],
      "metadata": {
        "id": "HXJyHyy-B7nE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some architecture as model_1 (but using nn.Sequential())\n",
        "model_2 = nn.Sequential(\n",
        "    nn.Linear(in_features=1, out_features=10),\n",
        "    nn.Linear(in_features=10, out_features=10),\n",
        "    nn.Linear(in_features=10, out_features=1)\n",
        ").to(device)\n",
        "\n",
        "# Create a loss function and optimizer\n",
        "loss_fn = nn.L1Loss()\n",
        "optimizer = torch.optim.SGD(params=model_2.parameters(),\n",
        "                            lr=0.01)"
      ],
      "metadata": {
        "id": "gzqGA92uCNjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set the number of epochs\n",
        "epochs = 1000\n",
        "\n",
        "# Put the data on the target device\n",
        "X_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device)\n",
        "X_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)\n",
        "\n",
        "# Training\n",
        "for epoch in range(epochs):\n",
        "  y_pred = model_2(X_train_regression)\n",
        "  loss = loss_fn(y_pred, y_train_regression)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # testing\n",
        "  model_2.eval()\n",
        "  with torch.inference_mode():\n",
        "    test_pred = model_2(X_test_regression)\n",
        "    test_loss = loss_fn(test_pred, y_test_regression)\n",
        "  # Print our what's happening\n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"Epoch: {epoch} | Loss: {loss:.5f} | Test Loss: {test_loss:.5f}\")\n"
      ],
      "metadata": {
        "id": "uLvZwq5HC1GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn on evaluation mode\n",
        "model_2.eval()\n",
        "\n",
        "# Make predictions (inference)\n",
        "with torch.inference_mode():\n",
        "  y_preds = model_2(X_test_regression)\n",
        "\n",
        "# Plot data and predictions\n",
        "plot_predictions(train_data=X_train_regression.cpu(),\n",
        "                 train_labels=y_train_regression.cpu(),\n",
        "                 test_data=X_test_regression.cpu(),\n",
        "                 test_labels=y_test_regression.cpu(),\n",
        "                 predictions=y_preds.cpu()); # Add .cpu() here"
      ],
      "metadata": {
        "id": "Ua-8_0z4EXce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. The missing piece : non-Linearity\n",
        "\n",
        "* What paterns could you draw if you were given an infinite amount of a straight and non-straight lines?\n",
        "\n",
        "Or in mahcine leanring terms, an infinite (but really it is finite) of linear and non-linear functions"
      ],
      "metadata": {
        "id": "6xTE1rCNFGkw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Recreating non-inear data (red and blue circles)"
      ],
      "metadata": {
        "id": "EG3RylqHGos-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make and plot data\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "n_samples = 1000\n",
        "\n",
        "X, y = make_circles(n_samples,\n",
        "                    noise=0.03,\n",
        "                    random_state=42)\n",
        "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu);"
      ],
      "metadata": {
        "id": "FeIkPbuvGw8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert fdata to tensors and then to train and test splits\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Turn data into tensors\n",
        "X = torch.from_numpy(X).type(torch.float)\n",
        "y = torch.from_numpy(y).type(torch.float)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train[:5], y_train[:5]"
      ],
      "metadata": {
        "id": "zljzCIrAHOFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Building a model with non-linearity\n",
        "\n",
        "* Linear = straight linear\n",
        "* Non-linear = non-straight lines\n",
        "\n",
        "Artificial neural networks are a large combination of linear (straight) and non-linear (non-straight) functions which are potentially able to find patterns in data"
      ],
      "metadata": {
        "id": "CiDSGn5FH4-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a model with non-linear activation functions\n",
        "from torch import nn\n",
        "class CircleModelV2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
        "    self.layer_2 = nn.Linear(in_features=10, out_features=10)\n",
        "    self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
        "    self.relu = nn.ReLU() # rectified linear unit\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Where should we put our non-linear activation functions?\n",
        "    return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n",
        "\n",
        "model_3 = CircleModelV2().to(device)\n",
        "model_3"
      ],
      "metadata": {
        "id": "ZQiu5nHOs7SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss and optimizer\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.SGD(params=model_3.parameters(),\n",
        "                            lr=0.1)"
      ],
      "metadata": {
        "id": "LM5soWJEvR2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Training a model with non-linearity"
      ],
      "metadata": {
        "id": "G-xX0wiowMfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Put all data on target device\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "# Loop through data\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  ### Training\n",
        "  model_3.train()\n",
        "\n",
        "  # Forward pass\n",
        "  y_logits = model_3(X_train).squeeze()\n",
        "  y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> prediction probabilities -> prediction label\n",
        "\n",
        "  # Calculate loss/acc\n",
        "  loss = loss_fn(y_logits, y_train)\n",
        "  acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
        "\n",
        "  # Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Loss backward (backpropagation)\n",
        "  loss.backward()\n",
        "\n",
        "  # Optimizer step\n",
        "  optimizer.step()\n",
        "\n",
        "  ### Testing\n",
        "  model_3.eval()\n",
        "  with torch.inference_mode():\n",
        "    # Forward pass\n",
        "    test_logits = model_3(X_test).squeeze()\n",
        "    test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "\n",
        "    test_loss = loss_fn(test_logits, y_test)\n",
        "    test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
        "  # Print out what's happening\n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"Epoch: {epoch} | Loss: {loss:.4f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.4f}, Test Acc: {test_acc:2f}%\")\n"
      ],
      "metadata": {
        "id": "XG8LpMcnwbdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 6.4 Evaluating a model trained with non-linear activation functions\n",
        "model_3.eval()\n",
        "with torch.inference_mode():\n",
        "  y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze()\n",
        "y_preds[:10], y_test[:10]\n"
      ],
      "metadata": {
        "id": "6fRy0bPExzWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot decision boundary\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_1, X_train, y_train) # model_1 = no non-linearity\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_3, X_test, y_test) # model_3 = has non-linearity"
      ],
      "metadata": {
        "id": "Qy-garO7y5JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Replicating non-linear acitvation functions\n",
        "\n",
        "Neural networks, rather than us telling the model what to learn, we give it the tools to discover patterns in data and it tries to figure out the patterns on its own\n",
        "\n",
        "And these tools are linear and non-linear functions."
      ],
      "metadata": {
        "id": "5QUDx0AXzEon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor\n",
        "A = torch.arange(-10, 10, 1, dtype=torch.float32)\n",
        "A.dtype"
      ],
      "metadata": {
        "id": "Wf9tfHkaz-0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the tensor\n",
        "plt.plot(A);"
      ],
      "metadata": {
        "id": "0FHYY1Y60Myw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(torch.relu(A));"
      ],
      "metadata": {
        "id": "MXazDNY_0YWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "  return torch.maximum(torch.tensor(0), x) # inputs must be tensors\n",
        "\n",
        "relu(A)"
      ],
      "metadata": {
        "id": "4Wn7j1UN0cy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ReLU activation function\n",
        "plt.plot(relu(A))"
      ],
      "metadata": {
        "id": "DPceCLMk0t7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's do the same for Sigmoid\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + torch.exp(-x))"
      ],
      "metadata": {
        "id": "Y_YjeeTh08y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(torch.sigmoid(A));"
      ],
      "metadata": {
        "id": "32IKV9h31XF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(sigmoid(A));"
      ],
      "metadata": {
        "id": "YWl02ZR41b3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Putting it all together with a multi-class classification problem\n",
        "\n",
        "* Binary classification = one thing or another (cat vs dog, spam vs not spam, etc.)\n",
        "* Multi-class classification = more than one thing or another (cat vs dog vs chicken)"
      ],
      "metadata": {
        "id": "hR6NuVgT1iPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 Creating a toy multi-class dataset"
      ],
      "metadata": {
        "id": "M4kIJg2N5s_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set the hyperparameters for data collection\n",
        "NUM_CLASSES = 4\n",
        "NUM_FEATURES = 2\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# 1. Create multi-class data\n",
        "X_blob, y_blob = make_blobs(n_samples=1000,\n",
        "                            n_features=NUM_FEATURES,\n",
        "                            centers=NUM_CLASSES,\n",
        "                            cluster_std=1.5, # give the clusters a little shake up\n",
        "                            random_state=RANDOM_SEED)\n",
        "\n",
        "# 2. Turn data into tensors\n",
        "X_blob = torch.from_numpy(X_blob).type(torch.float)\n",
        "y_blob = torch.from_numpy(y_blob).type(torch.float)\n",
        "\n",
        "# Split into train and test\n",
        "X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob, y_blob, test_size=0.2, random_state=RANDOM_SEED)\n",
        "\n",
        "# 4. Plot data (visualize)\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.scatter(X_blob[:,0], X_blob[:,1], c=y_blob, cmap=plt.cm.RdYlBu);"
      ],
      "metadata": {
        "id": "M9rlFjku6Icq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 Building a multi-class classification model in PyTorch\n"
      ],
      "metadata": {
        "id": "sepaoSHj8pTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "VftfRQIG86DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a multi-class classification model\n",
        "class BlobModel(nn.Module):\n",
        "  def __init__(self, input_features, output_features, hidden_units=8):\n",
        "    \"\"\" Initializes multi-class classification model\n",
        "\n",
        "    Args:\n",
        "    input_features (int): Number of input features to the model\n",
        "    output_features (int): Number of outputs features (number ofoutput classes)\n",
        "    hidden_units (int): Number of hidden units between layers, default 8\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    Example:\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.linear_layer_stack = nn.Sequential(\n",
        "        nn.Linear(in_features=input_features, out_features=hidden_units),\n",
        "        # nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
        "        # nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units, out_features=output_features)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.linear_layer_stack(x)\n",
        "\n",
        "# Create an instance of BlobModel and send it to the target device\n",
        "model_4 = BlobModel(input_features=2,\n",
        "                    output_features=4,\n",
        "                    hidden_units=8).to(device)\n",
        "model_4"
      ],
      "metadata": {
        "id": "Lt7bxdfV9XP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3 Create a loss function and an optimizer for a multi-class classification model"
      ],
      "metadata": {
        "id": "dAaVqWK3-oV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a loss function and optimizer for multi-class classification\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_4.parameters(), # optimizer updates our model parameters to try and reduce the loss\n",
        "                            lr=0.1)\n"
      ],
      "metadata": {
        "id": "dFO6xIJR-q3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.4 Getting prediction probabilities for a multi-class PyTorch model\n",
        "\n",
        "In order to evaluate and train and test our model,  we need to convert our model's outputs (logits) to prediction probabilities and then to prediction labels\n",
        "\n",
        "Logits (raw output of the model) -> Pred probs (use 'torch.softmax') -> Pred labels (take the argmax of the prediction probabilties)"
      ],
      "metadata": {
        "id": "QpPFAwq5Awli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get some raw output of our model (logits)\n",
        "model_4.eval()\n",
        "with torch.inference_mode():\n",
        "  y_logits = model_4(X_blob_test.to(device))\n",
        "y_logits[:10]"
      ],
      "metadata": {
        "id": "GZu4jm82A2S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_blob_test[:10]"
      ],
      "metadata": {
        "id": "l19Ie_N-0aZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert our model's logit outputs to prediction probabilties\n",
        "y_pred_probs = torch.softmax(y_logits, dim=1)\n",
        "print(y_logits[:5])\n",
        "print(y_pred_probs[:5])"
      ],
      "metadata": {
        "id": "EcmikXkB0fyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert our model's prediction probabilities to prediction labels\n",
        "y_preds = torch.argmax(y_pred_probs, dim=1)\n",
        "y_preds[:10]"
      ],
      "metadata": {
        "id": "p2O_QqNQ1gFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_blob_test[:10]"
      ],
      "metadata": {
        "id": "abzNlQMF2NdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5 Create a training loop and testing loop for a multi-class PyTorch model\n"
      ],
      "metadata": {
        "id": "bkhWOVRc2Y3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the multi-class model to the data\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set number of epochs\n",
        "epochs = 100\n",
        "\n",
        "# Put data to the target device\n",
        "X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device).long()\n",
        "X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device).long()\n",
        "\n",
        "# Loop through data\n",
        "for epoch in range(epochs):\n",
        "  ### Training\n",
        "  model_4.train()\n",
        "\n",
        "  # Forward pass\n",
        "  y_logits = model_4(X_blob_train.to(device)) # Ensure input data is on the correct device\n",
        "  y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # logits -> prediction probabilities -> prediction labels\n",
        "\n",
        "  # Calculate loss/acc\n",
        "  loss = loss_fn(y_logits, y_blob_train.to(device)) # Ensure target labels are on the correct device\n",
        "  acc = accuracy_fn(y_true=y_blob_train.to(device), y_pred=y_pred)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  ### Testing\n",
        "  model_4.eval()\n",
        "  with torch.inference_mode():\n",
        "    test_logits = model_4(X_blob_test.to(device)) # Ensure test data is on the correct device\n",
        "    test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
        "    test_loss = loss_fn(test_logits, y_blob_test.to(device)) # Ensure test labels are on the correct device\n",
        "    test_acc = accuracy_fn(y_true=y_blob_test.to(device), y_pred=test_pred)\n",
        "  # Print out what's happening\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: {epoch} | Loss: {loss:.4f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "2DB6h8yi3WsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.6 Making and evaluating predictions with a PyTorch multi-class model"
      ],
      "metadata": {
        "id": "fC5fdjRP7XRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_4.eval()\n",
        "with torch.inference_mode():\n",
        "  y_logits = model_4(X_blob_test.to(device))\n",
        "# Vuew the first 10 predictions\n",
        "y_logits[:10]"
      ],
      "metadata": {
        "id": "lkqqJwYx7glp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_blob_test"
      ],
      "metadata": {
        "id": "nyFRmv7r78Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Go from pred prob to pred labels\n",
        "y_preds = torch.argmax(y_pred_probs, dim=1)\n",
        "y_preds[:10]"
      ],
      "metadata": {
        "id": "WGUmIJmP8JXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d509a6b"
      },
      "source": [
        "# Plot decision boundary of the multi-class model\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_4, X_blob_train, y_blob_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_4, X_blob_test, y_blob_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. A few more classification metrics (to evaluate our classification model)\n",
        "\n",
        "* Accuracy - out of our 100 samples, how many of them do we get right\n",
        "* Precision\n",
        "* Recall\n",
        "* F1-score\n",
        "* Confusion matric\n",
        "* Classification report"
      ],
      "metadata": {
        "id": "lX_nOF7P9Ch8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "airu7Zt29eph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import Accuracy\n",
        "\n",
        "# Setup metric\n",
        "torchmetric_accuracy = Accuracy(task=\"multiclass\", num_classes=4).to(device)\n",
        "\n",
        "# Calculate accuracy\n",
        "torchmetric_accuracy(y_preds, y_blob_test)"
      ],
      "metadata": {
        "id": "ZBmd0tr2-Wrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jg5zPFFf-n3m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}