{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM/+XrCG+WaBYTNe7DlRMzv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chpham92/chpham92/blob/main/08_pytorch_paper_replicating_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 08. Milestone Project: PyTorch Paper Replicating\n",
        "\n",
        "The goal of machine learning research paper is: turn a ML research paper into usable code.\n",
        "\n",
        "In this notebook, we're going to be replicating the Vision Transformer (ViT) architecture/paper with PyTorch"
      ],
      "metadata": {
        "id": "_DiiNWMAH43V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Get setup"
      ],
      "metadata": {
        "id": "IH0S4svEIrfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "id": "9qYk2ZgYIwA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, os.getcwd()) # Ensure current directory is in sys.path for imports\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    # Re-attempt imports after download\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ],
      "metadata": {
        "id": "rsv345eQI22V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "RMrkqt2KJLNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Get data:\n",
        "\n"
      ],
      "metadata": {
        "id": "rIGYOnk9JeQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pizza, steak, sushi images from GitHub\n",
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")\n",
        "image_path"
      ],
      "metadata": {
        "id": "g9_1cuRjKHYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup directory paths to train and test images\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ],
      "metadata": {
        "id": "4uA7fsfLKICE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Create Datasets and DataLoaders"
      ],
      "metadata": {
        "id": "jHVqYxJKKymr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from going_modular.going_modular import data_setup, engine\n",
        "\n",
        "# Create image size (from Table 3 in the ViT paper)\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Create transform pipeline manually\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "print(f\"Manually created transforms: {manual_transforms}\")"
      ],
      "metadata": {
        "id": "tJKDKi7BLD3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the batch size\n",
        "BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=manual_transforms, # use manually created transforms\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "ZzfaReuALx93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader), len(test_dataloader)"
      ],
      "metadata": {
        "id": "ODWOBJ3TMHpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Visualize a single image"
      ],
      "metadata": {
        "id": "UApmHMpEMZ_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of images\n",
        "image_batch, label_batch = next(iter(train_dataloader))\n",
        "\n",
        "# Get a single image from the batch\n",
        "image, label = image_batch[0], label_batch[0]\n",
        "\n",
        "# View the batch shapes\n",
        "image.shape, label"
      ],
      "metadata": {
        "id": "ut7xGo8mMjBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot image with matplotlib\n",
        "plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -> [height, width, color_channels]\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "DL5TejTIMq0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Replicating ViT Overview\n",
        "\n",
        "ViT is a deep learning neural network architecture.\n",
        "\n",
        "And any neural network architecture is generally comprised of layers.\n",
        "\n",
        "And a collection of layers is often referred to as a block.\n",
        "\n",
        "And stacking many blocks together is what gives us the whole architecture.\n",
        "\n",
        "* Layer - takes an input, performs a function on it, returns an output.\n",
        "* Block - a collection of layers, takes an input, performs a series of functions on it, returns an output.\n",
        "* Architecture (or model) - a collection of blocks, takes an input, performs a series of functions on it, returns an output."
      ],
      "metadata": {
        "id": "N9LpHMwcMxTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What's ViT made of?\n",
        "\n",
        "The three main rsources we'll be looking at for tha architecture design is:\n",
        "\n",
        "1. Figure 1 - This gives an overview of the model in a graphical sense, you could almost recreate the architecture with this figure alone.\n",
        "2. Four equations in section 3.1 - These equations give a little bit more of a mathematical grounding to the coloured blocks in Figure 1.\n",
        "3. Table 1 - This table shows the various hyperparameter settings (such as number of layers and number of hidden units) for different ViT model variants. We'll be focused on the smallest version, ViT-Base."
      ],
      "metadata": {
        "id": "VMoscHoOIc3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Figure 1\n",
        "![](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-figure-1-architecture-overview.png)\n",
        "#### Four equations\n",
        "![](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-four-equations.png)\n",
        "#### Table 1\n",
        "![](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-table-1.png)"
      ],
      "metadata": {
        "id": "bT-KQRuUJ5dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.  Equation 1: Split data into patches and creating the class, position and patch embedding\n",
        "\n",
        "Layers = input -> function -> output\n",
        "\n",
        "* Input shape: $H\\times{W}\\times{C}$ (height, width, color channels)\n",
        "* Output shape: ${N \\times\\left(P^{2} \\cdot C\\right)}$\n",
        "* H = height\n",
        "* W = width\n",
        "* C = color channels\n",
        "* P = patch size\n",
        "* N = number of patches = (height x width) / p^2\n",
        "* D = constant latents vector size = embeddingdimension (see Table 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "lGSeWuTkL3dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create example values\n",
        "height = 224 # H (\"The training resolution is 224.\")\n",
        "width = 224 # W\n",
        "color_channels = 3 # C\n",
        "patch_size = 16 # P\n",
        "\n",
        "# Calculate N (number of patches)\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "print(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\")"
      ],
      "metadata": {
        "id": "ojcLTqAkQvMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input shape (this is the size of a single image)\n",
        "embedding_layer_input_shape = (height, width, color_channels)\n",
        "\n",
        "# Output shape\n",
        "embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)\n",
        "\n",
        "print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\n",
        "print(f\"Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}\")"
      ],
      "metadata": {
        "id": "yW7vZpYEQ_Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Turning a single image into patches"
      ],
      "metadata": {
        "id": "OJWILaHcRtOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View single image\n",
        "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "uNUYuo_qSKwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change image shape to be compatible with matplotlib (color_channels, height, width) -> (height, width, color_channels)\n",
        "image_permuted = image.permute(1, 2, 0)\n",
        "\n",
        "# Index to plot the top row of patched pixels\n",
        "patch_size = 16\n",
        "plt.figure(figsize=(patch_size, patch_size))\n",
        "plt.imshow(image_permuted[:patch_size, :, :]);"
      ],
      "metadata": {
        "id": "tzA-nJ6LSV0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup hyperparameters and make sure img_size and patch_size are compatible\n",
        "img_size = 224\n",
        "patch_size = 16\n",
        "num_patches = img_size/patch_size\n",
        "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
        "print(f\"Number of patches per row: {num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
        "\n",
        "# Create a series of subplots\n",
        "fig, axs = plt.subplots(nrows=1,\n",
        "                        ncols=img_size // patch_size, # one column for each patch\n",
        "                        figsize=(num_patches, num_patches),\n",
        "                        sharex=True,\n",
        "                        sharey=True)\n",
        "\n",
        "# Iterate through number of patches in the top row\n",
        "for i, patch in enumerate(range(0, img_size, patch_size)):\n",
        "    axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # keep height index constant, alter the width index\n",
        "    axs[i].set_xlabel(i+1) # set the label\n",
        "    axs[i].set_xticks([])\n",
        "    axs[i].set_yticks([])"
      ],
      "metadata": {
        "id": "SvFlhxO0TINU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup hyperparameters and make sure img_size and patch_size are compatible\n",
        "img_size = 224\n",
        "patch_size = 16\n",
        "num_patches = img_size/patch_size\n",
        "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
        "print(f\"Number of patches per row: {num_patches}\\\n",
        "        \\nNumber of patches per column: {num_patches}\\\n",
        "        \\nTotal patches: {num_patches*num_patches}\\\n",
        "        \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
        "\n",
        "# Create a series of subplots\n",
        "fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float\n",
        "                        ncols=img_size // patch_size,\n",
        "                        figsize=(num_patches, num_patches),\n",
        "                        sharex=True,\n",
        "                        sharey=True)\n",
        "\n",
        "# Loop through height and width of image\n",
        "for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height\n",
        "    for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width\n",
        "\n",
        "        # Plot the permuted image patch (image_permuted -> (Height, Width, Color Channels))\n",
        "        axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height\n",
        "                                        patch_width:patch_width+patch_size, # iterate through width\n",
        "                                        :]) # get all color channels\n",
        "\n",
        "        # Set up label information, remove the ticks for clarity and set labels to outside\n",
        "        axs[i, j].set_ylabel(i+1,\n",
        "                             rotation=\"horizontal\",\n",
        "                             horizontalalignment=\"right\",\n",
        "                             verticalalignment=\"center\")\n",
        "        axs[i, j].set_xlabel(j+1)\n",
        "        axs[i, j].set_xticks([])\n",
        "        axs[i, j].set_yticks([])\n",
        "        axs[i, j].label_outer()\n",
        "\n",
        "# Set a super title\n",
        "fig.suptitle(f\"{class_names[label]} -> Patchified\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jzrC29VCT-A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Creating image patches and turning them into patch embeddings\n",
        "\n",
        "We could create the image patches and image patch embeddings in a single step using `torch.nn.Conv2D()` and setting the kernel size and stride parameters to `patch_size`"
      ],
      "metadata": {
        "id": "ZWlcz3ooWA6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "# Set the patch size\n",
        "patch_size=16\n",
        "\n",
        "# Create the Conv2d layer with hyperparameters from the ViT paper\n",
        "conv2d = nn.Conv2d(in_channels=3, # number of color channels\n",
        "                   out_channels=768, # from Table 1: Hidden size D, this is the embedding size\n",
        "                   kernel_size=patch_size, # could also use (patch_size, patch_size)\n",
        "                   stride=patch_size,\n",
        "                   padding=0)"
      ],
      "metadata": {
        "id": "DibVno5mXqT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View single image\n",
        "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "v3BTHY9uaEfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the image through the convolutional layer\n",
        "image_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -> (batch, height, width, color_channels)\n",
        "print(image_out_of_conv.shape)"
      ],
      "metadata": {
        "id": "NRISjKlIaGoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot random 5 convolutional feature maps\n",
        "import random\n",
        "random_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size\n",
        "print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n",
        "\n",
        "# Create plot\n",
        "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))\n",
        "\n",
        "# Plot random image feature maps\n",
        "for i, idx in enumerate(random_indexes):\n",
        "    image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer\n",
        "    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())\n",
        "    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]);"
      ],
      "metadata": {
        "id": "7TmR-e4gaIsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a single feature map in tensor form\n",
        "single_feature_map = image_out_of_conv[:, 0, :, :]\n",
        "single_feature_map, single_feature_map.requires_grad"
      ],
      "metadata": {
        "id": "pHXUA-Kkb-Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Flattening the patch embedding with `torch.nn.Flatten()`"
      ],
      "metadata": {
        "id": "AI7xWMl0cOI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Current tensor shape\n",
        "print(f\"Current tensor shape: {image_out_of_conv.shape} -> [batch, embedding_dim, feature_map_height, feature_map_width]\")"
      ],
      "metadata": {
        "id": "VEX4oD_9cwvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create flatten layer\n",
        "flatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)\n",
        "                     end_dim=3) # flatten feature_map_width (dimension 3)"
      ],
      "metadata": {
        "id": "3ne_8gZlW2YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. View single image\n",
        "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);\n",
        "print(f\"Original image shape: {image.shape}\")\n",
        "\n",
        "# 2. Turn image into feature maps\n",
        "image_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors\n",
        "print(f\"Image feature map shape: {image_out_of_conv.shape}\")\n",
        "\n",
        "# 3. Flatten the feature maps\n",
        "image_out_of_conv_flattened = flatten(image_out_of_conv)\n",
        "print(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")"
      ],
      "metadata": {
        "id": "wq5Pex9ZW3ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get flattened image patch embeddings in right shape\n",
        "image_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]\n",
        "print(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -> [batch_size, num_patches, embedding_size]\")"
      ],
      "metadata": {
        "id": "kzE6SKpcXRGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a single flattened feature map\n",
        "single_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0] # index: (batch_size, number_of_patches, embedding_dimension)\n",
        "\n",
        "# Plot the flattened feature map visually\n",
        "plt.figure(figsize=(22, 22))\n",
        "plt.imshow(single_flattened_feature_map.detach().numpy())\n",
        "plt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "FYLLNVYVXRuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the flattened feature map as a tensor\n",
        "single_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape"
      ],
      "metadata": {
        "id": "FQDmCSf-XYiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Turning the ViT patch embedding layer into a PyTorch module\n",
        "\n",
        "Time to put everything we've done for creating the patch embedding into a single PyTorch layer.\n",
        "\n",
        "We can do so by subclassing `nn.Module` and creating a small PyTorch \"model\" to do all of the steps above.\n",
        "\n",
        "Specifically we'll:\n",
        "\n",
        "1. Create a class called `PatchEmbedding` which subclasses `nn.Module` (so it can be used a PyTorch layer).\n",
        "2. Initialize the class with the parameters `in_channels=3, patch_size=16` (for ViT-Base) and `embedding_dim=768` (this is $D$ for ViT-Base from Table 1).\n",
        "3. Create a layer to turn an image into patches using `nn.Conv2d()` (just like in 4.3 above).\n",
        "4. Create a layer to flatten the patch feature maps into a single dimension (just like in 4.4 above).\n",
        "5. Define a `forward()` method to take an input and pass it through the layers created in 3 and 4.\n",
        "6. Make sure the output shape reflects the required output shape of the ViT architecture (${N \\times\\left(P^{2} \\cdot C\\right)}$)."
      ],
      "metadata": {
        "id": "N0dxTS3_XiAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class which subclasses nn.Module\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of color channels for the input images. Defaults to 3.\n",
        "        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n",
        "        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n",
        "    \"\"\"\n",
        "    # 2. Initialize the class with appropriate variables\n",
        "    def __init__(self,\n",
        "                 in_channels:int=3,\n",
        "                 patch_size:int=16,\n",
        "                 embedding_dim:int=768):\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create a layer to turn an image into patches\n",
        "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
        "                                 out_channels=embedding_dim,\n",
        "                                 kernel_size=patch_size,\n",
        "                                 stride=patch_size,\n",
        "                                 padding=0)\n",
        "\n",
        "        # 4. Create a layer to flatten the patch feature maps into a single dimension\n",
        "        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n",
        "                                  end_dim=3)\n",
        "\n",
        "    # 5. Define the forward method\n",
        "    def forward(self, x):\n",
        "        # Create assertion to check that inputs are the correct shape\n",
        "        image_resolution = x.shape[-1]\n",
        "        assert image_resolution % patch_size == 0, f\"Input image size must be divisible by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n",
        "\n",
        "        # Perform the forward pass\n",
        "        x_patched = self.patcher(x)\n",
        "        x_flattened = self.flatten(x_patched)\n",
        "        # 6. Make sure the output shape has the right order\n",
        "        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]"
      ],
      "metadata": {
        "id": "Naz12vLDXpDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds()\n",
        "\n",
        "# Create an instance of patch embedding layer\n",
        "patchify = PatchEmbedding(in_channels=3,\n",
        "                          patch_size=16,\n",
        "                          embedding_dim=768)\n",
        "\n",
        "# Pass a single image through\n",
        "print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n",
        "patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\n",
        "print(f\"Output patch embedding shape: {patch_embedded_image.shape}\")"
      ],
      "metadata": {
        "id": "N1z596uMYdXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output shape matches the ideal input and output shapes we'd like to see from the patch embedding layer:\n",
        "\n",
        "Input: The image starts as 2D with size ${H \\times W \\times C}$.\n",
        "Output: The image gets converted to a 1D sequence of flattened 2D patches with size ${N \\times\\left(P^{2} \\cdot C\\right)}$.\n",
        "Where:\n",
        "\n",
        "* $(H, W)$ is the resolution of the original image.\n",
        "* $C$ is the number of channels.\n",
        "* $(P, P)$ is the resolution of each image patch (patch size).\n",
        "* $N=H W / P^{2}$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer.\n",
        "\n",
        "We've now replicated the patch embedding for equation 1 but not the class token/position embedding.\n",
        "\n",
        "We'll get to these later on."
      ],
      "metadata": {
        "id": "MXPxhsBoYiAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create random input sizes\n",
        "random_input_image = (1, 3, 224, 224)\n",
        "random_input_image_error = (1, 3, 250, 250) # will error because image size is incompatible with patch_size\n",
        "\n",
        "# # Get a summary of the input and outputs of PatchEmbedding (uncomment for full output)\n",
        "# summary(PatchEmbedding(),\n",
        "#         input_size=random_input_image, # try swapping this for \"random_input_image_error\"\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "VrmsXNruY9TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6 Creating the class token embedding\n",
        "\n",
        "Okay we've made the image patch embedding, time to get to work on the class token embedding.\n",
        "\n",
        "\n",
        "Or $\\mathbf{x}_\\text {class }$ from equation 1.\n",
        "\n",
        "Next we need to \"preprend a learnable embedding to the sequence of embedded patches\".\n",
        "\n",
        "* prepend = adding to the front"
      ],
      "metadata": {
        "id": "IYQOQcSDimka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View the patch embedding and patch embedding shape\n",
        "print(patch_embedded_image)\n",
        "print(f\"Patch embedding shape: {patch_embedded_image.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "EaGigkP_is_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To \"prepend a learnable embedding to the sequence of embedded patches\" we need to create a learnable embedding in the shape of the `embedding_dimension` ($D$) and then add it to the `number_of_patches` dimension."
      ],
      "metadata": {
        "id": "0_NhZwBGjR5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the batch size and embedding dimension\n",
        "batch_size = patch_embedded_image.shape[0]\n",
        "embedding_dimension = patch_embedded_image.shape[-1]\n",
        "\n",
        "# Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)\n",
        "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]\n",
        "                           requires_grad=True) # make sure the embedding is learnable\n",
        "\n",
        "# Show the first 10 examples of the class_token\n",
        "print(class_token[:, :, :10])\n",
        "\n",
        "# Print the class_token shape\n",
        "print(f\"Class token shape: {class_token.shape} -> [batch_size, number_of_tokens, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "qILjmdBIjbR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the class token embedding to the front of the patch embedding\n",
        "patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),\n",
        "                                                      dim=1) # concat on first dimension\n",
        "\n",
        "# Print the sequence of patch embeddings with the prepended class token embedding\n",
        "print(patch_embedded_image_with_class_embedding)\n",
        "print(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "vlMSLuILjt_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.7 Creating the position embedding"
      ],
      "metadata": {
        "id": "JMr0RR-JjyPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View the sequence of patch embeddings with the prepended class embedding\n",
        "patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape"
      ],
      "metadata": {
        "id": "5RHxEUEwj1kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate N (number of patches)\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "\n",
        "# Get embedding dimension\n",
        "embedding_dimension = patch_embedded_image_with_class_embedding.shape[2]\n",
        "\n",
        "# Create the learnable 1D position embedding\n",
        "position_embedding = nn.Parameter(torch.ones(1,\n",
        "                                             number_of_patches+1,\n",
        "                                             embedding_dimension),\n",
        "                                  requires_grad=True) # make sure it's learnable\n",
        "\n",
        "# Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding\n",
        "print(position_embedding[:, :10, :10])\n",
        "print(f\"Position embedding shape: {position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "hpHo-bLCvs4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the position embedding to the patch and class token embedding\n",
        "patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\n",
        "print(patch_and_position_embedding)\n",
        "print(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "zym5OjDdv2Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.8 Putting it all together: from image to embedding\n",
        "\n",
        "Alright, we've come a long way in terms of turning our input images into an embedding and replicating equation 1 from section 3.1 of the ViT paper:\n",
        "\n",
        "$$ \\begin{aligned} \\mathbf{z}_{0} &=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, & & \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D} \\end{aligned} $$\n",
        "\n",
        "Let's now put everything together in a single code cell and go from input image ($\\mathbf{x}$) to output embedding ($\\mathbf{z}_0$).\n",
        "\n",
        "We can do so by:\n",
        "\n",
        "1. Setting the patch size (we'll use 16 as it's widely used throughout the paper and for ViT-Base).\n",
        "2. Getting a single image, printing its shape and storing its height and width.\n",
        "3. Adding a batch dimension to the single image so it's compatible with our `PatchEmbedding` layer.\n",
        "4. Creating a `PatchEmbedding` layer (the one we made in section 4.5) with a `patch_size=16` and embedding_dim=768 (from Table 1 for ViT-Base).\n",
        "5. Passing the single image through the `PatchEmbedding` layer in 4 to create a sequence of patch embeddings.\n",
        "6. Creating a class token embedding like in section 4.6.\n",
        "7. Prepending the class token embedding to the patch embeddings created in step 5.\n",
        "8. Creating a position embedding like in section 4.7.\n",
        "9. Adding the position embedding to the class token and patch embeddings created in step 7.\n",
        "\n",
        "We'll also make sure to set the random seeds with `set_seeds()` and print out the shapes of different tensors along the way."
      ],
      "metadata": {
        "id": "DgLjF8KUwcSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds()\n",
        "\n",
        "# 1. Set patch size\n",
        "patch_size = 16\n",
        "\n",
        "# 2. Print shape of original image tensor and get the image dimensions\n",
        "print(f\"Image tensor shape: {image.shape}\")\n",
        "height, width = image.shape[1], image.shape[2]\n",
        "\n",
        "# 3. Get image tensor and add batch dimension\n",
        "x = image.unsqueeze(0)\n",
        "print(f\"Input image with batch dimension shape: {x.shape}\")\n",
        "\n",
        "# 4. Create patch embedding layer\n",
        "patch_embedding_layer = PatchEmbedding(in_channels=3,\n",
        "                                       patch_size=patch_size,\n",
        "                                       embedding_dim=768)\n",
        "\n",
        "# 5. Pass image through patch embedding layer\n",
        "patch_embedding = patch_embedding_layer(x)\n",
        "print(f\"Patching embedding shape: {patch_embedding.shape}\")\n",
        "\n",
        "# 6. Create class token embedding\n",
        "batch_size = patch_embedding.shape[0]\n",
        "embedding_dimension = patch_embedding.shape[-1]\n",
        "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n",
        "                           requires_grad=True) # make sure it's learnable\n",
        "print(f\"Class token embedding shape: {class_token.shape}\")\n",
        "\n",
        "# 7. Prepend class token embedding to patch embedding\n",
        "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
        "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n",
        "\n",
        "# 8. Create position embedding\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n",
        "                                  requires_grad=True) # make sure it's learnable\n",
        "\n",
        "# 9. Add position embedding to patch embedding with class token\n",
        "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
        "print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")"
      ],
      "metadata": {
        "id": "Vt_5qOYzwduc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Equation 2. Multi-Head Self Attention (MSA)\n",
        "\n",
        "* Multihead self-attention: which part of a sequence should pay the most attention to itself?\n",
        "\n",
        "* In our case we have a series of embedded images patches, which patch significantly relates to another patch\n",
        "\n",
        "* LayerNorn = layer normalization is a technique to normalize the distribution of intermediate layers. It enables smoother gradients, training,  and better generalization accuracy.\n",
        "\n",
        "* Normalization = make everything have the same mean and same standard deviation\n",
        "\n"
      ],
      "metadata": {
        "id": "CTVkz49gyLG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class MultiheadSelfAttentionBlock(nn.Module):\n",
        "    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n",
        "    \"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
        "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
        "                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create the Norm layer (LN)\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "        # 4. Create the Multi-Head Attention (MSA) layer\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
        "                                                    num_heads=num_heads,\n",
        "                                                    dropout=attn_dropout,\n",
        "                                                    batch_first=True) # does our batch dimension come first?\n",
        "\n",
        "    # 5. Create a forward() method to pass the data through the layers\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        attn_output, _ = self.multihead_attn(query=x, # query embeddings\n",
        "                                             key=x, # key embeddings\n",
        "                                             value=x, # value embeddings\n",
        "                                             need_weights=False) # do we need the weights or just the layer outputs?\n",
        "        return attn_output"
      ],
      "metadata": {
        "id": "gk6bPXVsqMvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of MSABlock\n",
        "multihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1\n",
        "                                                             num_heads=12) # from Table 1\n",
        "\n",
        "# Pass patch and position image embedding through MSABlock\n",
        "patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\n",
        "print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\n",
        "print(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")"
      ],
      "metadata": {
        "id": "V2KvYB2_h3Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Equation 3: Multilayer Perception\n",
        "\n",
        "Let's keep it going and replicate equation 3:\n",
        "\n",
        "$$ \\begin{aligned} \\mathbf{z}_{\\ell} &=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, & & \\ell=1 \\ldots L \\end{aligned} $$\n",
        "\n",
        "Here MLP stands for \"multilayer perceptron\" and LN stands for \"layer normalization\" (as discussed above).\n",
        "\n",
        "And the addition on the end is the skip/residual connection.\n",
        "\n",
        "We'll refer to equation 3 as the \"MLP block\" of the Transformer encoder"
      ],
      "metadata": {
        "id": "h6cUXsIyh6av"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replicating Equation 3 with PyTorch layers\n",
        "\n",
        "1. Create a class called `MLPBlock` that inherits from `torch.nn.Module`.\n",
        "2. Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper for the ViT-Base model.\n",
        "3. Create a layer normalization (LN) layer with `torch.nn.LayerNorm()` with the `normalized_shape` parameter the same as our embedding dimension ($D$ from Table 1).\n",
        "4. Create a sequential series of MLP layers(s) using `torch.nn.Linear()`, `torch.nn.Dropout()` and `torch.nn.GELU()` with appropriate hyperparameter values from Table 1 and Table 3.\n",
        "5. Create a `forward()` method for our class passing the in the inputs through the LN layer and MLP layer(s)."
      ],
      "metadata": {
        "id": "T7mY6Sdoi624"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class MLPBlock(nn.Module):\n",
        "    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n",
        "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
        "                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create the Norm layer (LN)\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "        # 4. Create the Multilayer perceptron (MLP) layer(s)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_features=embedding_dim,\n",
        "                      out_features=mlp_size),\n",
        "            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n",
        "                      out_features=embedding_dim), # take back to embedding_dim\n",
        "            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n",
        "        )\n",
        "\n",
        "    # 5. Create a forward() method to pass the data through the layers\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "wy6RKJQej0BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of MLPBlock\n",
        "mlp_block = MLPBlock(embedding_dim=768, # from Table 1\n",
        "                     mlp_size=3072, # from Table 1\n",
        "                     dropout=0.1) # from Table 3\n",
        "\n",
        "# Pass output of MSABlock through MLPBlock\n",
        "patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\n",
        "print(f\"Input shape of MLP block: {patched_image_through_msa_block.shape}\")\n",
        "print(f\"Output shape MLP block: {patched_image_through_mlp_block.shape}\")"
      ],
      "metadata": {
        "id": "fEGchPPgkgsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Create the Transformer Encoder\n",
        "The Transformer Encoder is a combination of alternating blocks of MSA and MLP.\n",
        "\n",
        "Time to stack together our `MultiheadSelfAttentionBlock` (equation 2) and `MLPBlock` (equation 3) and create the Transformer Encoder of the ViT architecture.\n",
        "\n",
        "To do so, we'll:\n",
        "\n",
        "1. Create a class called `TransformerEncoderBlock` that inherits from `torch.nn.Module`.\n",
        "2. Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper for the ViT-Base model.\n",
        "3. Instantiate a MSA block for equation 2 using our `MultiheadSelfAttentionBlock` from section 5.2 with the appropriate parameters.\n",
        "4. Instantiate a MLP block for equation 3 using our `MLPBlock` from section 6.2 with the appropriate parameters.\n",
        "5. Create a `forward()` method for our `TransformerEncoderBlock` class.\n",
        "6. Create a residual connection for the MSA block (for equation 2).\n",
        "7. Create a residual connection for the MLP block (for equation 3)."
      ],
      "metadata": {
        "id": "BgDgNom4klbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"Creates a Transformer Encoder block.\"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
        "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
        "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
        "                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
        "                 attn_dropout:float=0): # Amount of dropout for attention layers\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create MSA block (equation 2)\n",
        "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
        "                                                     num_heads=num_heads,\n",
        "                                                     attn_dropout=attn_dropout)\n",
        "\n",
        "        # 4. Create MLP block (equation 3)\n",
        "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
        "                                   mlp_size=mlp_size,\n",
        "                                   dropout=mlp_dropout)\n",
        "\n",
        "    # 5. Create a forward() method\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 6. Create residual connection for MSA block (add the input to the output)\n",
        "        x =  self.msa_block(x) + x\n",
        "\n",
        "        # 7. Create residual connection for MLP block (add the input to the output)\n",
        "        x = self.mlp_block(x) + x\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "tZC5xud5kvE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of TransformerEncoderBlock\n",
        "transformer_encoder_block = TransformerEncoderBlock()\n",
        "\n",
        "# # Print an input and output summary of our Transformer Encoder (uncomment for full output)\n",
        "# summary(model=transformer_encoder_block,\n",
        "#         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "T7W60JlClogw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2 Creating a Transformer Encoder with PyTorch's Transformer layers"
      ],
      "metadata": {
        "id": "bIWuGWIzlv61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the same as above with torch.nn.TransformerEncoderLayer()\n",
        "torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768, # Hidden size D from Table 1 for ViT-Base\n",
        "                                                             nhead=12, # Heads from Table 1 for ViT-Base\n",
        "                                                             dim_feedforward=3072, # MLP size from Table 1 for ViT-Base\n",
        "                                                             dropout=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
        "                                                             activation=\"gelu\", # GELU non-linear activation\n",
        "                                                             batch_first=True, # Do our batches come first?\n",
        "                                                             norm_first=True) # Normalize first or after MSA/MLP layers?\n",
        "\n",
        "torch_transformer_encoder_layer"
      ],
      "metadata": {
        "id": "D1d-64jVl-sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Get the output of PyTorch's version of the Transformer Encoder (uncomment for full output)\n",
        "# summary(model=torch_transformer_encoder_layer,\n",
        "#         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "IQ3bMYhEmBKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why spend all this time recreating the transformer encoder when we could've just made it with a single PyTorch layer?\n",
        "\n",
        "For Praactice.\n",
        "\n",
        "Now we know how things are implemented behind the scenes, we can tweak the if necessary.\n",
        "\n",
        "But there are benefits of using the PyTorch pre-built layers, such as:\n",
        "\n",
        "* **Less prone to errors** - Generally, if a layer makes it into the PyTorch standard library, it's been tested and tried to work.\n",
        "* **Potentially better performance** - As of July 2022 and PyTorch 1.12, the PyTorch implemented version of `torch.nn.TransformerEncoderLayer()` can see a speedup of more than 2x on many common workloads."
      ],
      "metadata": {
        "id": "qSBTMsqj3EK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Putting it all together to create ViT\n",
        "\n",
        "But now it's time to do the exciting thing of putting together all of the pieces of the puzzle.\n",
        "\n",
        "We're going to combine all of the blocks we've created to replicate the full ViT architecture.\n",
        "\n",
        "From the patch and positional embedding to the Transformer Encoder(s) to the MLP Head.\n",
        "\n",
        "But wait, we haven't created equation 4 yet...\n",
        "\n",
        "$$ \\begin{aligned} \\mathbf{y} &=\\operatorname{LN}\\left(\\mathbf{z}_{L}^{0}\\right) & & \\end{aligned} $$\n",
        "\n",
        "Don't worry, we can put equation 4 into our overall ViT architecture class.\n",
        "\n",
        "All we need is a torch.nn.LayerNorm() layer and a torch.nn.Linear() layer to convert the 0th index ($\\mathbf{z}_{L}^{0}$) of the Transformer Encoder logit outputs to the target number of classes we have.\n",
        "\n",
        "To create the full architecture, we'll also need to stack a number of our `TransformerEncoderBlock`s on top of each other, we can do this by passing a list of them to `torch.nn.Sequential()` (this will make a sequential range of `TransformerEncoderBlock`s)."
      ],
      "metadata": {
        "id": "a0tgjPLHmGRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To bring our own implementation of ViT to life, let's:\n",
        "\n",
        "1. Create a class called ViT that inherits from torch.nn.Module.\n",
        "2. Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper for the ViT-Base model.\n",
        "3. Make sure the image size is divisible by the patch size (the image should be split into even patches).\n",
        "4. Calculate the number of patches using the formula $N=H W / P^{2}$, where $H$ is the image height, $W$ is the image width and $P$ is the patch size.\n",
        "5. Create a learnable class embedding token (equation 1) as done above in section 4.6.\n",
        "6. Create a learnable position embedding vector (equation 1) as done above in section 4.7.\n",
        "7. Setup the embedding dropout layer as discussed in Appendix B.1 of the ViT paper.\n",
        "8. Create the patch embedding layer using the PatchEmbedding class as above in section 4.5.\n",
        "9. Create a series of Transformer Encoder blocks by passing a list of TransformerEncoderBlocks created in section 7.1 to torch.nn.Sequential() (equations 2 & 3).\n",
        "10. Create the MLP head (also called classifier head or equation 4) by passing a torch.nn.LayerNorm() (LN) layer and a torch.nn.Linear(out_features=num_classes) layer (where num_classes is the target number of classes) linear layer to torch.nn.Sequential().\n",
        "11. Create a forward() method that accepts an input.\n",
        "12. Get the batch size of the input (the first dimension of the shape).\n",
        "13. Create the patching embedding using the layer created in step 8 (equation 1).\n",
        "14. Create the class token embedding using the layer created in step 5 and expand it across the number of batches found in step 11 using torch.Tensor.expand() (equation 1).\n",
        "15. Concatenate the class token embedding created in step 13 to the first dimension of the patch embedding created in step 12 using torch.cat() (equation 1).\n",
        "16. Add the position embedding created in step 6 to the patch and class token embedding created in step 14 (equation 1).\n",
        "17. Pass the patch and position embedding through the dropout layer created in step 7.\n",
        "18. Pass the patch and position embedding from step 16 through the stack of Transformer Encoder layers created in step 9 (equations 2 & 3).\n",
        "19. Pass index 0 of the output of the stack of Transformer Encoder layers from step 17 through the classifier head created in step 10 (equation 4)."
      ],
      "metadata": {
        "id": "j1sokeXSmOiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a ViT class that inherits from nn.Module\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
        "    def __init__(self,\n",
        "                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n",
        "                 in_channels:int=3, # Number of channels in input image\n",
        "                 patch_size:int=16, # Patch size\n",
        "                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
        "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
        "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
        "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
        "                 attn_dropout:float=0, # Dropout for attention projection\n",
        "                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers\n",
        "                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n",
        "                 num_classes:int=1000): # Default for ImageNet but can customize this\n",
        "        super().__init__() # don't forget the super().__init__()!\n",
        "\n",
        "        # 3. Make the image size is divisible by the patch size\n",
        "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
        "\n",
        "        # 4. Calculate number of patches (height * width/patch^2)\n",
        "        self.num_patches = (img_size * img_size) // patch_size**2\n",
        "\n",
        "        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n",
        "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
        "                                            requires_grad=True)\n",
        "\n",
        "        # 6. Create learnable position embedding\n",
        "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
        "                                               requires_grad=True)\n",
        "\n",
        "        # 7. Create embedding dropout value\n",
        "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
        "\n",
        "        # 8. Create patch embedding layer\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
        "                                              patch_size=patch_size,\n",
        "                                              embedding_dim=embedding_dim)\n",
        "\n",
        "        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())\n",
        "        # Note: The \"*\" means \"all\"\n",
        "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
        "                                                                            num_heads=num_heads,\n",
        "                                                                            mlp_size=mlp_size,\n",
        "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
        "\n",
        "        # 10. Create classifier head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "            nn.Linear(in_features=embedding_dim,\n",
        "                      out_features=num_classes)\n",
        "        )\n",
        "\n",
        "    # 11. Create a forward() method\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 12. Get batch size\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n",
        "        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n",
        "\n",
        "        # 14. Create patch embedding (equation 1)\n",
        "        x = self.patch_embedding(x)\n",
        "\n",
        "        # 15. Concat class embedding and patch embedding (equation 1)\n",
        "        x = torch.cat((class_token, x), dim=1)\n",
        "\n",
        "        # 16. Add position embedding to patch embedding (equation 1)\n",
        "        x = self.position_embedding + x\n",
        "\n",
        "        # 17. Run embedding dropout (Appendix B.1)\n",
        "        x = self.embedding_dropout(x)\n",
        "\n",
        "        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # 19. Put 0 index logit through classifier (equation 4)\n",
        "        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ILTGWchNnsun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit = Vit()\n",
        "vit"
      ],
      "metadata": {
        "id": "FvhjUaOV9qKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of creating the class embedding and expanding over a batch dimension\n",
        "batch_size = 32\n",
        "class_token_embedding_single = nn.Parameter(data=torch.randn(1, 1, 768)) # create a single learnable class token\n",
        "class_token_embedding_expanded = class_token_embedding_single.expand(batch_size, -1, -1) # expand the single learnable class token across the batch dimension, \"-1\" means to \"infer the dimension\"\n",
        "\n",
        "# Print out the change in shapes\n",
        "print(f\"Shape of class token embedding single: {class_token_embedding_single.shape}\")\n",
        "print(f\"Shape of class token embedding expanded: {class_token_embedding_expanded.shape}\")"
      ],
      "metadata": {
        "id": "iC_-C1-rnwaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds()\n",
        "\n",
        "# Create a random tensor with same shape as a single image\n",
        "random_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)\n",
        "\n",
        "# Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)\n",
        "vit = ViT(num_classes=len(class_names))\n",
        "\n",
        "# Pass the random image tensor to our ViT instance\n",
        "vit(random_image_tensor)"
      ],
      "metadata": {
        "id": "bKIoyMYKn5aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1 Getting a visual summary of our ViT model"
      ],
      "metadata": {
        "id": "Wqc2AOm0n8W-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# # Print a summary of our custom ViT model using torchinfo (uncomment for actual output)\n",
        "# summary(model=vit,\n",
        "#         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
        "#         # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"]\n",
        "# )"
      ],
      "metadata": {
        "id": "q5b3c3AGoDjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Setting up training code for our ViT model"
      ],
      "metadata": {
        "id": "OhtfyKtVoFNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.1 Creating an optimizer\n",
        "\n",
        "The authors set Adam's $\\beta$ (beta) values to $\\beta_{1}=0.9, \\beta_{2}=0.999$, these are the default values for the betas parameter in `torch.optim.Adam(betas=(0.9, 0.999))`.\n",
        "\n",
        "They also state the use of weight decay (slowly reducing the values of the weights during optimization to prevent overfitting), we can set this with the `weight_decay` parameter in `torch.optim.Adam(weight_decay=0.3)` (according to the setting of ViT-* trained on ImageNet-1k).\n",
        "\n",
        "We'll set the learning rate of the optimizer to 0.003 as per Table 3 (according to the setting of ViT-* trained on ImageNet-1k).\n",
        "\n",
        "And as discussed previously, we're going to use a lower batch size than 4096 due to hardware limitations (if you have a large GPU, feel free to increase this).\n",
        "\n"
      ],
      "metadata": {
        "id": "EtHyen50oQZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2 Creating a loss function\n",
        "\n",
        "Since the target problem we're working with is multi-class classification (the same for the ViT paper), we'll use `torch.nn.CrossEntropyLoss()`."
      ],
      "metadata": {
        "id": "vlmSRKf7o9ZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.3 Training our ViT model"
      ],
      "metadata": {
        "id": "knKbVvhipJVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper\n",
        "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
        "                             lr=3e-3, # Base LR from Table 3 for ViT-* ImageNet-1k\n",
        "                             betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training & Fine-tuning)\n",
        "                             weight_decay=0.3) # from the ViT paper section 4.1 (Training & Fine-tuning) and Table 3 for ViT-* ImageNet-1k\n",
        "\n",
        "# Setup the loss function for multi-class classification\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Set the seeds\n",
        "set_seeds()\n",
        "\n",
        "# Train the model and save the training results to a dictionary\n",
        "results = engine.train(model=vit,\n",
        "                       train_dataloader=train_dataloader,\n",
        "                       test_dataloader=test_dataloader,\n",
        "                       optimizer=optimizer,\n",
        "                       loss_fn=loss_fn,\n",
        "                       epochs=10,\n",
        "                       device=device)"
      ],
      "metadata": {
        "id": "3XpBr-ixoViy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.4 What our training setup is missing\n",
        "\n",
        "Prevent underfitting:\n",
        "* Data - our setup uses far less data (225 vs millions)\n",
        "\n",
        "* Prevent overfitting\n",
        "* Learning rate warmup - start with a low learning rate and increase to a base LR\n",
        "* Learning rate decay - as your model gets closer to convergence, start to lower the learning rate\n",
        "* Gradient clipping - prevent gradients from getting too big"
      ],
      "metadata": {
        "id": "aE0END0wEZBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.5 Plot the loss curves of our ViT model"
      ],
      "metadata": {
        "id": "hO__XuiNoawE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "# Plot our ViT model's loss curves\n",
        "plot_loss_curves(results)"
      ],
      "metadata": {
        "id": "X4_sYF4epd3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Using a pretrained ViT from torchvision.models on the same datset\n",
        "\n"
      ],
      "metadata": {
        "id": "ySZYlngEpfqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting a pretrained ViT model and creating a feature extractor"
      ],
      "metadata": {
        "id": "2eE1zJIVp4X2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following requires torch v0.12+ and torchvision v0.13+\n",
        "import torch\n",
        "import torchvision\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "id": "-88zIwAlp0tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "YYk14Yf-p2Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Finally, we'll get the pretrained ViT-Base with patch size 16 from torchvision.models and prepare it for our FoodVision Mini use case by turning it into a feature extractor transfer learning model.\n",
        "\n",
        "Specifically, we'll:\n",
        "\n",
        "1. Get the pretrained weights for ViT-Base trained on ImageNet-1k from torchvision.models.ViT_B_16_Weights.DEFAULT (DEFAULT stands for best available).\n",
        "2. Setup a ViT model instance via torchvision.models.vit_b_16, pass it the pretrained weights step 1 and send it to the target device.\n",
        "3. Freeze all of the parameters in the base ViT model created in step 2 by setting their requires_grad attribute to False.\n",
        "4. Update the classifier head of the ViT model created in step 2 to suit our own problem by changing the number of out_features to our number of classes (pizza, steak, sushi)."
      ],
      "metadata": {
        "id": "7O6it2pBqNzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Get pretrained weights for ViT-Base\n",
        "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision >= 0.13, \"DEFAULT\" means best available\n",
        "\n",
        "# 2. Setup a ViT model instance with pretrained weights\n",
        "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
        "\n",
        "# 3. Freeze the base parameters\n",
        "for parameter in pretrained_vit.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "# 4. Change the classifier head (set the seeds to ensure same initialization with linear head)\n",
        "set_seeds()\n",
        "pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n",
        "# pretrained_vit # uncomment for model output"
      ],
      "metadata": {
        "id": "8fmjOTs-qeVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Print a summary using torchinfo (uncomment for actual output)\n",
        "# summary(model=pretrained_vit,\n",
        "#         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
        "#         # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"]\n",
        "# )"
      ],
      "metadata": {
        "id": "afmqosc9qiBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.3 Preparing data for the pretrained ViT model"
      ],
      "metadata": {
        "id": "Ew8buX7oqlhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import download_data\n",
        "\n",
        "# Download pizza, steak, sushi images from GitHub\n",
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")\n",
        "image_path"
      ],
      "metadata": {
        "id": "Hx2k6Wxlqv76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup train and test directory paths\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "id": "ZJi9VqSyqx49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get automatic transforms from pretrained ViT weights\n",
        "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
        "print(pretrained_vit_transforms)"
      ],
      "metadata": {
        "id": "WXA7AvNZq0wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup dataloaders\n",
        "train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                                     test_dir=test_dir,\n",
        "                                                                                                     transform=pretrained_vit_transforms,\n",
        "                                                                                                     batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n"
      ],
      "metadata": {
        "id": "tLKCB-tfq3PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.4 Train feature extractor ViT model"
      ],
      "metadata": {
        "id": "ggcdOy__q5xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Create optimizer and loss function\n",
        "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n",
        "                             lr=1e-3)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the classifier head of the pretrained ViT feature extractor model\n",
        "set_seeds()\n",
        "pretrained_vit_results = engine.train(model=pretrained_vit,\n",
        "                                      train_dataloader=train_dataloader_pretrained,\n",
        "                                      test_dataloader=test_dataloader_pretrained,\n",
        "                                      optimizer=optimizer,\n",
        "                                      loss_fn=loss_fn,\n",
        "                                      epochs=10,\n",
        "                                      device=device)"
      ],
      "metadata": {
        "id": "J6SnmL9xq_bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.5 Plot feature extractor ViT model loss curves"
      ],
      "metadata": {
        "id": "siHVyuVirBKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss curves\n",
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(pretrained_vit_results)"
      ],
      "metadata": {
        "id": "czwIYbfArI0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.6 Save feature extractor ViT model and check file size"
      ],
      "metadata": {
        "id": "p_sZn7_krM-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "from going_modular.going_modular import utils\n",
        "\n",
        "utils.save_model(model=pretrained_vit,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\")"
      ],
      "metadata": {
        "id": "rec8V8U1rXoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\n",
        "print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")"
      ],
      "metadata": {
        "id": "QGDz-xqKrZAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Make predictions on a custom image"
      ],
      "metadata": {
        "id": "pl9hEi00rc67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Import function to make predictions on images and plot them\n",
        "from going_modular.going_modular.predictions import pred_and_plot_image\n",
        "\n",
        "# Setup custom image path\n",
        "custom_image_path = image_path / \"04-pizza-dad.jpeg\"\n",
        "\n",
        "# Download the image if it doesn't already exist\n",
        "if not custom_image_path.is_file():\n",
        "    with open(custom_image_path, \"wb\") as f:\n",
        "        # When downloading from GitHub, need to use the \"raw\" file link\n",
        "        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n",
        "        print(f\"Downloading {custom_image_path}...\")\n",
        "        f.write(request.content)\n",
        "else:\n",
        "    print(f\"{custom_image_path} already exists, skipping download.\")\n",
        "\n",
        "# Predict on custom image\n",
        "pred_and_plot_image(model=pretrained_vit,\n",
        "                    image_path=custom_image_path,\n",
        "                    class_names=class_names)"
      ],
      "metadata": {
        "id": "yIwMuTMVrkmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AVllDWGdrmq3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}